nohup: ignoring input
05/16/2024 13:55:47 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: False
05/16/2024 13:55:47 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_pin_memory=True,
ddp_backend=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
generation_config=None,
generation_max_length=None,
generation_num_beams=None,
gradient_accumulation_steps=16,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.02,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/Fin-Train-chatglm2-6b-pt-128-2e-2/runs/May16_13-55-47_VM-72-17-tencentos,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=10,
logging_strategy=steps,
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=600,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_hf,
optim_args=None,
output_dir=output/Fin-Train-chatglm2-6b-pt-128-2e-2,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=1,
per_device_train_batch_size=1,
predict_with_generate=True,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=output/Fin-Train-chatglm2-6b-pt-128-2e-2,
save_on_each_node=False,
save_safetensors=False,
save_steps=100,
save_strategy=steps,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
sortish_sampler=False,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
xpu_backend=None,
)
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/datasets/load.py:2516: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.
You can remove this warning by passing 'token=<use_auth_token>' instead.
  warnings.warn(
[INFO|configuration_utils.py:667] 2024-05-16 13:55:48,460 >> loading configuration file ../../data/ZhipuAI/chatglm2-6b/config.json
[INFO|configuration_utils.py:667] 2024-05-16 13:55:48,461 >> loading configuration file ../../data/ZhipuAI/chatglm2-6b/config.json
[INFO|configuration_utils.py:725] 2024-05-16 13:55:48,462 >> Model config ChatGLMConfig {
  "_name_or_path": "../../data/ZhipuAI/chatglm2-6b/",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMModel"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1e-05,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_layers": 28,
  "original_rope": true,
  "pad_token_id": 0,
  "padded_vocab_size": 65024,
  "post_layer_norm": true,
  "pre_seq_len": null,
  "prefix_projection": false,
  "quantization_bit": 0,
  "rmsnorm": true,
  "seq_length": 32768,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.30.2",
  "use_cache": true,
  "vocab_size": 65024
}

[INFO|tokenization_utils_base.py:1821] 2024-05-16 13:55:48,464 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:1821] 2024-05-16 13:55:48,464 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:1821] 2024-05-16 13:55:48,464 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:1821] 2024-05-16 13:55:48,464 >> loading file tokenizer_config.json
[INFO|modeling_utils.py:2575] 2024-05-16 13:55:48,556 >> loading weights file ../../data/ZhipuAI/chatglm2-6b/pytorch_model.bin.index.json
[INFO|configuration_utils.py:577] 2024-05-16 13:55:48,557 >> Generate config GenerationConfig {
  "_from_model_config": true,
  "eos_token_id": 2,
  "pad_token_id": 0,
  "transformers_version": "4.30.2"
}

Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]Loading checkpoint shards:  14%|█▍        | 1/7 [00:00<00:02,  2.25it/s]Loading checkpoint shards:  29%|██▊       | 2/7 [00:00<00:02,  2.19it/s]Loading checkpoint shards:  43%|████▎     | 3/7 [00:01<00:01,  2.20it/s]Loading checkpoint shards:  57%|█████▋    | 4/7 [00:01<00:01,  2.24it/s]Loading checkpoint shards:  71%|███████▏  | 5/7 [00:02<00:00,  2.22it/s]Loading checkpoint shards:  86%|████████▌ | 6/7 [00:02<00:00,  2.22it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  2.52it/s]Loading checkpoint shards: 100%|██████████| 7/7 [00:02<00:00,  2.34it/s]
[INFO|modeling_utils.py:3295] 2024-05-16 13:55:51,718 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[WARNING|modeling_utils.py:3297] 2024-05-16 13:55:51,719 >> Some weights of ChatGLMForConditionalGeneration were not initialized from the model checkpoint at ../../data/ZhipuAI/chatglm2-6b/ and are newly initialized: ['transformer.prefix_encoder.embedding.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|modeling_utils.py:2927] 2024-05-16 13:55:51,720 >> Generation config file not found, using a generation config created from the model config.
Quantized to 8 bit
05/16/2024 13:55:51 - WARNING - transformers_modules.quantization - Failed to load cpm_kernels:No module named 'cpm_kernels'
Running tokenizer on train dataset (num_proc=10):   0%|          | 0/1041 [00:00<?, ? examples/s]Running tokenizer on train dataset (num_proc=10):  10%|█         | 105/1041 [00:00<00:03, 243.29 examples/s]Running tokenizer on train dataset (num_proc=10): 100%|██████████| 1041/1041 [00:00<00:00, 1883.80 examples/s]
input_ids [64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 54622, 38628, 30944, 826, 10945, 36954, 31936, 31698, 31123, 54622, 44715, 30944, 826, 10945, 36954, 54530, 13966, 35089, 40536, 31123, 44531, 31793, 54757, 36593, 54670, 54653, 31201, 54952, 55005, 54653, 54542, 32053, 34555, 32184, 40536, 13966, 35089, 13, 48112, 54670, 54653, 31211, 25558, 30962, 6100, 13, 48112, 54952, 55005, 54653, 54331, 31629, 54580, 55037, 31201, 43489, 31201, 32087, 31651, 34918, 41242, 54603, 54769, 31201, 33031, 32977, 33820, 31201, 44081, 31201, 54720, 54745, 54455, 31201, 44814, 32534, 39505, 31201, 54933, 33958, 33469, 31201, 39748, 39475, 31201, 32157, 53300, 31201, 32457, 47000, 31201, 42255, 32457, 31201, 33881, 32295, 31201, 32640, 39009, 31201, 37237, 32175, 31201, 41781, 31201, 33875, 54675, 35459, 31201, 56439, 55277, 39009, 31201, 33875, 33469, 31201, 33875, 54675, 32175, 31201, 40881, 32974, 31201, 31722, 33603, 31201, 34918, 54643, 34918, 54609, 54899, 54679, 55589, 31917, 55447, 31201, 36979, 31201, 31722, 52835, 55115, 31201, 33875, 32449, 31201, 31914, 33603, 36591, 31201, 33949, 32457, 39475, 31201, 52835, 56092, 55115, 31201, 55135, 37809, 54794, 31201, 31722, 54720, 37809, 31201, 54836, 33949, 32457, 39475, 31201, 31845, 48835, 33603, 31201, 32719, 32808, 31201, 31985, 40407, 32488, 54530, 34918, 31201, 44814, 56092, 55115, 31201, 33949, 39748, 39475, 31201, 33469, 36591, 31201, 31641, 32974, 31201, 31722, 33949, 32457, 31201, 56035, 55506, 40881, 32457, 31201, 33425, 32974, 31201, 33875, 54788, 32175, 31201, 54836, 33949, 39748, 39475, 31201, 55043, 55466, 31201, 33958, 55128, 54733, 31201, 33469, 54746, 56197, 55329, 37237, 32488, 54530, 34918, 31201, 52455, 48835, 33603, 31201, 31754, 54631, 33580, 39475, 31201, 33875, 54788, 32449, 31201, 32172, 32974, 31201, 39748, 54542, 31754, 54631, 33580, 47000, 31201, 31954, 32087, 36979, 31201, 33375, 55325, 55019, 33545, 31201, 33425, 31698, 31201, 32172, 31698, 31201, 31780, 33603, 31201, 31932, 31698, 31201, 46707, 31201, 37237, 32974, 31201, 31752, 31698, 31201, 32569, 32974, 31201, 32457, 55325, 55019, 33545, 31201, 56035, 55506, 33603, 31201, 31722, 54836, 33949, 32457, 31201, 35676, 38323, 31201, 38737, 43975, 32734, 39475, 30996, 13, 31937, 54570, 31639, 31697, 32977, 32224, 31301, 8073, 55212, 31201, 8073, 33639, 31201, 8073, 54809, 31300, 31636, 38434, 32224, 35731, 31123, 54627, 31211, 31623, 55212, 31201, 54531, 55212, 54828, 35731, 54541, 30939, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 31123, 54531, 33639, 54828, 35731, 54541, 30939, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 31123, 54778, 33639, 54828, 35731, 54541, 30943, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 31123, 54531, 35165, 54828, 35731, 54541, 30939, 30940, 30940, 30940, 30940, 30940, 30940, 31123, 54865, 33639, 54828, 35731, 54541, 30970, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 13, 31696, 13966, 35089, 31697, 54952, 55005, 54653, 52554, 48112, 54952, 55005, 54653, 31123, 32003, 33220, 54952, 55005, 54653, 13, 13, 54825, 55071, 41699, 31211, 13, 18427, 13, 32053, 34555, 31211, 30943, 30940, 30939, 30969, 54540, 50993, 33031, 39748, 39475, 32143, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 30910, 13, 10719, 42594, 54580, 55037, 13, 5823, 1441, 30962, 6100, 13, 3015, 422, 30910, 39748, 39475, 1721, 13, 24609, 30910, 30939, 13, 10846, 31040, 13, 13, 32053, 34555, 31211, 36938, 32750, 54530, 35544, 54538, 31123, 30943, 30940, 30939, 30969, 54540, 52825, 39748, 39475, 32143, 31514, 34309, 54532, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 30910, 13, 10719, 42594, 54580, 55037, 30932, 30910, 39748, 39475, 13, 5823, 1441, 30962, 6100, 13, 3079, 30910, 32750, 32675, 18538, 8246, 765, 31016, 31770, 31016, 30953, 765, 13, 394, 32605, 55093, 542, 765, 30943, 30940, 30939, 30969, 30953, 13, 3015, 422, 30910, 39748, 39475, 1721, 13, 24609, 30910, 30939, 13, 10846, 31040, 13, 13, 32053, 34555, 31211, 30943, 30940, 30939, 30969, 54540, 39748, 39475, 36948, 54707, 44238, 36733, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 30910, 13, 10719, 42594, 54580, 55037, 13, 5823, 1441, 30962, 6100, 13, 3079, 32605, 55093, 542, 765, 30943, 30940, 30939, 30969, 30953, 13, 3015, 422, 30910, 39748, 39475, 1721, 13, 24609, 30910, 30939, 30940, 13, 10846, 31040, 13, 13, 32053, 34555, 31211, 36938, 32750, 54530, 35544, 54538, 31123, 30943, 30940, 30939, 30969, 54540, 39748, 39475, 37631, 54707, 44238, 36733, 31123, 39748, 39475, 34309, 36733, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 30910, 13, 10719, 42594, 54580, 55037, 30932, 30910, 39748, 39475, 13, 5823, 1441, 30962, 6100, 13, 3079, 30910, 32750, 32675, 18538, 8246, 765, 31016, 31770, 31016, 30953, 30910, 13, 394, 32605, 55093, 542, 765, 30943, 30940, 30939, 30969, 30953, 13, 3015, 422, 30910, 39748, 39475, 1721, 13, 24609, 30910, 30939, 30940, 13, 13, 13, 32053, 34555, 31211, 32750, 33426, 54534, 35850, 37755, 54538, 31123, 30943, 30940, 30943, 30939, 54540, 39748, 39475, 40103, 54865, 33639, 37755, 54090, 54561, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 13, 2606, 1044, 30946, 30939, 30945, 13, 428, 1441, 30962, 6100, 13, 812, 32605, 55093, 542, 765, 30943, 30940, 30943, 30939, 30953, 293, 30910, 32750, 32675, 659, 765, 31016, 35850, 31016, 30953, 13, 293, 30910, 39748, 39475, 323, 430, 4346, 293, 30910, 39748, 39475, 2046, 30910, 30970, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 30910, 13, 10846, 31040, 13, 13, 13, 32053, 34555, 31211, 51413, 54746, 32611, 32750, 37755, 54538, 31123, 30943, 30940, 30943, 30940, 54540, 55043, 55466, 40103, 54707, 55212, 37755, 54090, 54561, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 13, 2606, 1044, 30946, 30939, 30945, 13, 428, 1441, 30962, 6100, 13, 812, 32605, 55093, 542, 765, 30943, 30940, 30943, 30940, 30953, 293, 359, 32750, 32675, 659, 765, 31016, 32370, 31016, 30953, 400, 30910, 32750, 32675, 659, 765, 31016, 32611, 31016, 3387, 13, 293, 30910, 55043, 55466, 323, 430, 4346, 293, 30910, 55043, 55466, 2046, 30910, 30939, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 30940, 30910, 13, 10846, 31040, 13, 13, 13, 32053, 34555, 31211, 32750, 33426, 54534, 32508, 37755, 54538, 31123, 30943, 30940, 30939, 30969, 52187, 54530, 33469, 36591, 44842, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 13, 2606, 1094, 30927, 30946, 33469, 36591, 30945, 13, 428, 1441, 30962, 6100, 13, 812, 32605, 55093, 542, 765, 30943, 30940, 30939, 30969, 30953, 293, 30910, 32750, 32675, 659, 765, 31016, 32508, 31016, 30953, 13, 293, 30910, 33469, 36591, 323, 430, 4346, 265, 13, 10846, 31040, 13, 13, 32053, 34555, 31211, 30943, 30940, 30943, 30939, 54540, 32750, 54563, 36938, 54530, 35544, 54538, 31123, 54531, 33684, 32326, 32172, 31698, 31514, 13, 13, 13966, 33163, 31211, 13, 10846, 31040, 13966, 13, 2606, 1991, 30946, 32172, 31698, 30945, 13, 428, 1441, 30962, 6100, 13, 812, 32605, 55093, 542, 765, 30943, 30940, 30943, 30939, 30953, 293, 30910, 32750, 32675, 659, 765, 31016, 31770, 31016, 30953, 13, 293, 30910, 32172, 31698, 323, 430, 4346, 265, 13, 10846, 31040, 13, 13, 18427, 13, 55073, 31793, 32040, 32053, 34555, 31123, 35898, 13966, 35089, 31155, 13, 32053, 34555, 31682, 30943, 30940, 30943, 30939, 54540, 54788, 39748, 36948, 54616, 30972, 44238, 36733, 32235, 13, 13, 55437, 31211, 47383, 32053, 34555, 31639, 31123, 40536, 13966, 35089, 33163, 31211, 13, 10846, 31040, 13966, 13, 23450, 42594, 54580, 55037, 30932, 30910, 39748, 39475, 11533, 1441, 30962, 6100, 18768, 32605, 55093, 542, 765, 30943, 30940, 30943, 30939, 30953, 4587, 30910, 39748, 39475, 3393, 5553, 9261, 6172, 15930, 10406, 30910, 39748, 39475, 21709, 30942, 25563, 1998, 30910, 30939, 3820, 10686, 2747, 30910, 30966, 13, 10846, 31040, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
inputs [Round 1]

问：你是一名Mysql数据库开发人员，你精通Mysql数据库的sql代码编写，你需要根据已知的表名、字段名和用户输入的问题编写sql代码
已知表名：company_table
已知字段名：[公司全称、年份、经营活动现金流入小计、公司的中文简称、固定资产、应交税费、应付职工薪酬、未分配利润、负债合计、电子信箱、资产总计、无形资产、货币资金、资本公积、利息收入、营业收入、营业外支出、盈余公积、营业利润、营业外收入、所得税费用、其他收益、现金及现金等价物净增加额、净利润、其他应收款、营业成本、综合收益总额、流动资产合计、应收账款、预付款项、其他应付款、非流动资产合计、基本每股收益、购买商品、接受劳务支付的现金、应付账款、流动负债合计、利润总额、管理费用、其他流动资产、递延所得税资产、财务费用、营业总收入、非流动负债合计、存货、分配股利、利润或偿付利息支付的现金、稀释每股收益、所有者权益合计、营业总成本、销售费用、负债和所有者权益总计、持续经营净利润、信用减值损失、财务人员、销售人员、投资收益、行政人员、技术人员、利息费用、生产人员、研发费用、资产减值损失、递延收益、其他非流动资产、短期借款、在职员工的数量合计]
注意对问题中的中文数字（xx亿、xx千万、xx万）进行阿拉伯数字转换，如：一个亿、一亿需转换为100000000，一千万需转换为10000000，两千万需转换为20000000，一百万需转换为1000000，五千万需转换为50000000
要求sql代码中的字段名必须是已知字段名，不得新增字段名

示例模板：
"""
用户输入：2019年哪家公司的负债合计最高？

sql如下：
```sql 
select 公司全称
from company_table
order by 负债合计 desc
limit 1
```

用户输入：在上海注册的上市公司中，2019年谁的负债合计最高？金额是？

sql如下：
```sql 
select 公司全称, 负债合计
from company_table
where 注册地址 LIKE '%上海%' '
and 年份 = '2019'
order by 负债合计 desc
limit 1
```

用户输入：2019年负债合计最高的十家公司分别是？

sql如下：
```sql 
select 公司全称
from company_table
where 年份 = '2019'
order by 负债合计 desc
limit 10
```

用户输入：在上海注册的上市公司中，2019年负债合计最多的十家公司分别是，负债合计金额分别是？

sql如下：
```sql 
select 公司全称, 负债合计
from company_table
where 注册地址 LIKE '%上海%' 
and 年份 = '2019'
order by 负债合计 desc
limit 10


用户输入：注册地点在深圳市的公司中，2021年负债合计超过了五千万的公司有几家？

sql如下：
```sql
 select count(1)
 from company_table
 where 年份 = '2021' and 注册地址 like '%深圳市%'
 and 负债合计 is not null and 负债合计 > 50000000 
```


用户输入：在深圳或重庆注册的公司中，2020年存货超过了十亿的公司有几家？

sql如下：
```sql
 select count(1)
 from company_table
 where 年份 = '2020' and (注册地址 like '%深圳%' or 注册地址 like '%重庆%')
 and 存货 is not null and 存货 > 1000000000 
```


用户输入：注册地点在四川的公司中，2019年平均的利润总额是多少？

sql如下：
```sql
 select avg(利润总额)
 from company_table
 where 年份 = '2019' and 注册地址 like '%四川%'
 and 利润总额 is not null  
```

用户输入：2021年注册地在上海的上市公司中，一共有多少销售人员？

sql如下：
```sql
 select sum(销售人员)
 from company_table
 where 年份 = '2021' and 注册地址 like '%上海%'
 and 销售人员 is not null  
```

"""
请根据以下用户输入，输出sql代码。
用户输入：“2021年总负债最高的第4家公司分别是？”

答： 根据用户输入问题，编写sql代码如下：
```sql
SELECT 公司全称, 负债合计 FROM company_table WHERE 年份 = '2021' AND 负债合计 IS NOT NULL ORDER BY 负债合计 DESC LIMIT 1 OFFSET 3
```
label_ids [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 47383, 32053, 34555, 31639, 31123, 40536, 13966, 35089, 33163, 31211, 13, 10846, 31040, 13966, 13, 23450, 42594, 54580, 55037, 30932, 30910, 39748, 39475, 11533, 1441, 30962, 6100, 18768, 32605, 55093, 542, 765, 30943, 30940, 30943, 30939, 30953, 4587, 30910, 39748, 39475, 3393, 5553, 9261, 6172, 15930, 10406, 30910, 39748, 39475, 21709, 30942, 25563, 1998, 30910, 30939, 3820, 10686, 2747, 30910, 30966, 13, 10846, 31040, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]
labels 根据用户输入问题，编写sql代码如下：
```sql
SELECT 公司全称, 负债合计 FROM company_table WHERE 年份 = '2021' AND 负债合计 IS NOT NULL ORDER BY 负债合计 DESC LIMIT 1 OFFSET 3
```
05/16/2024 13:55:54 - WARNING - accelerate.utils.other - Detected kernel version 5.4.119, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:577] 2024-05-16 13:55:54,596 >> max_steps is given, it will override any value given in num_train_epochs
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1786] 2024-05-16 13:55:54,728 >> ***** Running training *****
[INFO|trainer.py:1787] 2024-05-16 13:55:54,728 >>   Num examples = 1,041
[INFO|trainer.py:1788] 2024-05-16 13:55:54,728 >>   Num Epochs = 10
[INFO|trainer.py:1789] 2024-05-16 13:55:54,728 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:1790] 2024-05-16 13:55:54,728 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:1791] 2024-05-16 13:55:54,728 >>   Gradient Accumulation steps = 16
[INFO|trainer.py:1792] 2024-05-16 13:55:54,728 >>   Total optimization steps = 600
[INFO|trainer.py:1793] 2024-05-16 13:55:54,729 >>   Number of trainable parameters = 1,835,008
  0%|          | 0/600 [00:00<?, ?it/s]05/16/2024 13:55:55 - WARNING - transformers_modules.modeling_chatglm - `use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
  0%|          | 1/600 [01:01<10:13:55, 61.49s/it]  0%|          | 2/600 [02:02<10:08:56, 61.10s/it]  0%|          | 3/600 [03:03<10:06:33, 60.96s/it]  1%|          | 4/600 [04:03<10:04:52, 60.89s/it]  1%|          | 5/600 [05:04<10:03:31, 60.86s/it]  1%|          | 6/600 [06:05<10:02:15, 60.83s/it]  1%|          | 7/600 [07:06<10:01:31, 60.86s/it]  1%|▏         | 8/600 [08:07<10:00:07, 60.82s/it]  2%|▏         | 9/600 [09:07<9:58:53, 60.80s/it]   2%|▏         | 10/600 [10:08<9:57:47, 60.79s/it]                                                    2%|▏         | 10/600 [10:08<9:57:47, 60.79s/it]  2%|▏         | 11/600 [11:09<9:56:38, 60.78s/it]  2%|▏         | 12/600 [12:10<9:55:26, 60.76s/it]  2%|▏         | 13/600 [13:11<9:54:46, 60.80s/it]  2%|▏         | 14/600 [14:11<9:53:29, 60.77s/it]  2%|▎         | 15/600 [15:12<9:52:17, 60.75s/it]  3%|▎         | 16/600 [16:13<9:51:08, 60.73s/it]  3%|▎         | 17/600 [17:13<9:50:00, 60.72s/it]  3%|▎         | 18/600 [18:14<9:49:16, 60.75s/it]  3%|▎         | 19/600 [19:15<9:48:00, 60.72s/it]  3%|▎         | 20/600 [20:16<9:47:33, 60.78s/it]                                                    3%|▎         | 20/600 [20:16<9:47:33, 60.78s/it]  4%|▎         | 21/600 [21:16<9:46:09, 60.74s/it]  4%|▎         | 22/600 [22:17<9:44:53, 60.72s/it]  4%|▍         | 23/600 [23:18<9:43:37, 60.69s/it]  4%|▍         | 24/600 [24:18<9:42:26, 60.67s/it]  4%|▍         | 25/600 [25:19<9:41:21, 60.66s/it]  4%|▍         | 26/600 [26:20<9:40:16, 60.66s/it]  4%|▍         | 27/600 [27:20<9:39:10, 60.65s/it]  5%|▍         | 28/600 [28:21<9:38:09, 60.65s/it]  5%|▍         | 29/600 [29:21<9:37:09, 60.65s/it]  5%|▌         | 30/600 [30:22<9:36:03, 60.64s/it]                                                    5%|▌         | 30/600 [30:22<9:36:03, 60.64s/it]  5%|▌         | 31/600 [31:23<9:35:02, 60.64s/it]  5%|▌         | 32/600 [32:23<9:34:01, 60.64s/it]  6%|▌         | 33/600 [33:24<9:33:06, 60.65s/it]  6%|▌         | 34/600 [34:25<9:32:00, 60.64s/it]  6%|▌         | 35/600 [35:25<9:30:56, 60.63s/it]  6%|▌         | 36/600 [36:26<9:29:46, 60.61s/it]  6%|▌         | 37/600 [37:26<9:28:45, 60.61s/it]  6%|▋         | 38/600 [38:27<9:27:40, 60.61s/it]  6%|▋         | 39/600 [39:28<9:26:40, 60.61s/it]  7%|▋         | 40/600 [40:28<9:25:35, 60.60s/it]                                                    7%|▋         | 40/600 [40:28<9:25:35, 60.60s/it]  7%|▋         | 41/600 [41:29<9:24:37, 60.60s/it]  7%|▋         | 42/600 [42:29<9:23:35, 60.60s/it]  7%|▋         | 43/600 [43:30<9:22:31, 60.59s/it]  7%|▋         | 44/600 [44:31<9:21:32, 60.60s/it]  8%|▊         | 45/600 [45:31<9:20:34, 60.60s/it]  8%|▊         | 46/600 [46:32<9:19:36, 60.61s/it]  8%|▊         | 47/600 [47:32<9:18:36, 60.61s/it]  8%|▊         | 48/600 [48:33<9:17:34, 60.61s/it]  8%|▊         | 49/600 [49:34<9:16:32, 60.60s/it]  8%|▊         | 50/600 [50:34<9:15:30, 60.60s/it]                                                    8%|▊         | 50/600 [50:34<9:15:30, 60.60s/it]  8%|▊         | 51/600 [51:35<9:14:22, 60.59s/it]  9%|▊         | 52/600 [52:35<9:13:16, 60.58s/it]  9%|▉         | 53/600 [53:36<9:12:13, 60.57s/it]  9%|▉         | 54/600 [54:37<9:11:17, 60.58s/it]  9%|▉         | 55/600 [55:37<9:10:16, 60.58s/it]  9%|▉         | 56/600 [56:38<9:09:16, 60.58s/it] 10%|▉         | 57/600 [57:38<9:08:16, 60.58s/it] 10%|▉         | 58/600 [58:39<9:07:10, 60.57s/it] 10%|▉         | 59/600 [59:39<9:06:12, 60.58s/it] 10%|█         | 60/600 [1:00:40<9:05:17, 60.59s/it]                                                     10%|█         | 60/600 [1:00:40<9:05:17, 60.59s/it] 10%|█         | 61/600 [1:01:41<9:04:19, 60.59s/it] 10%|█         | 62/600 [1:02:41<9:03:15, 60.59s/it] 10%|█         | 63/600 [1:03:42<9:02:19, 60.59s/it] 11%|█         | 64/600 [1:04:42<9:01:18, 60.59s/it] 11%|█         | 65/600 [1:05:43<9:00:15, 60.59s/it] 11%|█         | 66/600 [1:06:44<8:59:11, 60.58s/it] 11%|█         | 67/600 [1:07:44<8:58:10, 60.58s/it] 11%|█▏        | 68/600 [1:08:45<8:57:08, 60.58s/it] 12%|█▏        | 69/600 [1:09:45<8:56:06, 60.58s/it] 12%|█▏        | 70/600 [1:10:46<8:55:05, 60.58s/it]                                                     12%|█▏        | 70/600 [1:10:46<8:55:05, 60.58s/it] 12%|█▏        | 71/600 [1:11:46<8:54:04, 60.58s/it] 12%|█▏        | 72/600 [1:12:47<8:53:01, 60.57s/it] 12%|█▏        | 73/600 [1:13:48<8:52:04, 60.58s/it] 12%|█▏        | 74/600 [1:14:48<8:51:06, 60.58s/it] 12%|█▎        | 75/600 [1:15:49<8:50:06, 60.58s/it] 13%|█▎        | 76/600 [1:16:49<8:48:59, 60.57s/it] 13%|█▎        | 77/600 [1:17:50<8:48:05, 60.58s/it] 13%|█▎        | 78/600 [1:18:50<8:47:02, 60.58s/it] 13%|█▎        | 79/600 [1:19:51<8:45:53, 60.56s/it] 13%|█▎        | 80/600 [1:20:52<8:44:51, 60.56s/it]                                                     13%|█▎        | 80/600 [1:20:52<8:44:51, 60.56s/it] 14%|█▎        | 81/600 [1:21:52<8:43:49, 60.56s/it] 14%|█▎        | 82/600 [1:22:53<8:42:57, 60.57s/it] 14%|█▍        | 83/600 [1:23:53<8:41:57, 60.57s/it] 14%|█▍        | 84/600 [1:24:54<8:40:51, 60.57s/it] 14%|█▍        | 85/600 [1:25:54<8:39:47, 60.56s/it] 14%|█▍        | 86/600 [1:26:55<8:38:40, 60.55s/it] 14%|█▍        | 87/600 [1:27:55<8:37:40, 60.55s/it] 15%|█▍        | 88/600 [1:28:56<8:36:50, 60.57s/it] 15%|█▍        | 89/600 [1:29:57<8:35:54, 60.58s/it] 15%|█▌        | 90/600 [1:30:57<8:34:47, 60.56s/it]                                                     15%|█▌        | 90/600 [1:30:57<8:34:47, 60.56s/it] 15%|█▌        | 91/600 [1:31:58<8:33:42, 60.55s/it] 15%|█▌        | 92/600 [1:32:58<8:32:49, 60.57s/it] 16%|█▌        | 93/600 [1:33:59<8:31:39, 60.55s/it] 16%|█▌        | 94/600 [1:34:59<8:30:35, 60.54s/it] 16%|█▌        | 95/600 [1:36:00<8:29:32, 60.54s/it] 16%|█▌        | 96/600 [1:37:00<8:28:36, 60.55s/it] 16%|█▌        | 97/600 [1:38:01<8:27:34, 60.54s/it] 16%|█▋        | 98/600 [1:39:02<8:26:37, 60.55s/it] 16%|█▋        | 99/600 [1:40:02<8:25:34, 60.55s/it] 17%|█▋        | 100/600 [1:41:03<8:24:30, 60.54s/it]                                                      17%|█▋        | 100/600 [1:41:03<8:24:30, 60.54s/it][INFO|configuration_utils.py:458] 2024-05-16 15:36:57,890 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-100/config.json
[INFO|configuration_utils.py:364] 2024-05-16 15:36:57,890 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-100/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-16 15:36:57,898 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-100/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-16 15:36:57,898 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-16 15:36:57,898 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-100/special_tokens_map.json
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 17%|█▋        | 101/600 [1:42:03<8:23:34, 60.55s/it] 17%|█▋        | 102/600 [1:43:04<8:22:35, 60.55s/it] 17%|█▋        | 103/600 [1:44:04<8:21:32, 60.55s/it] 17%|█▋        | 104/600 [1:45:05<8:20:33, 60.55s/it] 18%|█▊        | 105/600 [1:46:05<8:19:31, 60.55s/it] 18%|█▊        | 106/600 [1:47:06<8:18:26, 60.54s/it] 18%|█▊        | 107/600 [1:48:06<8:17:22, 60.53s/it] 18%|█▊        | 108/600 [1:49:07<8:16:26, 60.54s/it] 18%|█▊        | 109/600 [1:50:08<8:15:22, 60.53s/it] 18%|█▊        | 110/600 [1:51:08<8:14:14, 60.52s/it]                                                      18%|█▊        | 110/600 [1:51:08<8:14:14, 60.52s/it] 18%|█▊        | 111/600 [1:52:09<8:13:09, 60.51s/it] 19%|█▊        | 112/600 [1:53:09<8:12:12, 60.52s/it] 19%|█▉        | 113/600 [1:54:10<8:11:11, 60.52s/it] 19%|█▉        | 114/600 [1:55:10<8:10:15, 60.53s/it] 19%|█▉        | 115/600 [1:56:11<8:09:17, 60.53s/it] 19%|█▉        | 116/600 [1:57:11<8:08:17, 60.53s/it] 20%|█▉        | 117/600 [1:58:12<8:07:29, 60.56s/it] 20%|█▉        | 118/600 [1:59:12<8:06:23, 60.55s/it] 20%|█▉        | 119/600 [2:00:13<8:05:22, 60.54s/it] 20%|██        | 120/600 [2:01:13<8:04:19, 60.54s/it]                                                      20%|██        | 120/600 [2:01:13<8:04:19, 60.54s/it] 20%|██        | 121/600 [2:02:14<8:03:24, 60.55s/it] 20%|██        | 122/600 [2:03:14<8:02:14, 60.53s/it] 20%|██        | 123/600 [2:04:15<8:01:11, 60.53s/it] 21%|██        | 124/600 [2:05:16<8:00:09, 60.53s/it] 21%|██        | 125/600 [2:06:16<7:59:06, 60.52s/it] 21%|██        | 126/600 [2:07:17<7:58:04, 60.52s/it] 21%|██        | 127/600 [2:08:17<7:57:02, 60.51s/it] 21%|██▏       | 128/600 [2:09:18<7:56:07, 60.52s/it] 22%|██▏       | 129/600 [2:10:18<7:55:02, 60.52s/it] 22%|██▏       | 130/600 [2:11:19<7:54:11, 60.53s/it]                                                      22%|██▏       | 130/600 [2:11:19<7:54:11, 60.53s/it] 22%|██▏       | 131/600 [2:12:19<7:53:09, 60.53s/it] 22%|██▏       | 132/600 [2:13:20<7:52:01, 60.52s/it] 22%|██▏       | 133/600 [2:14:20<7:51:03, 60.52s/it] 22%|██▏       | 134/600 [2:15:21<7:50:03, 60.52s/it] 22%|██▎       | 135/600 [2:16:21<7:49:00, 60.52s/it] 23%|██▎       | 136/600 [2:17:22<7:48:02, 60.52s/it] 23%|██▎       | 137/600 [2:18:22<7:46:55, 60.51s/it] 23%|██▎       | 138/600 [2:19:23<7:45:54, 60.51s/it] 23%|██▎       | 139/600 [2:20:23<7:44:58, 60.52s/it] 23%|██▎       | 140/600 [2:21:24<7:43:58, 60.52s/it]                                                      23%|██▎       | 140/600 [2:21:24<7:43:58, 60.52s/it] 24%|██▎       | 141/600 [2:22:24<7:43:02, 60.53s/it] 24%|██▎       | 142/600 [2:23:25<7:41:58, 60.52s/it] 24%|██▍       | 143/600 [2:24:25<7:41:00, 60.53s/it] 24%|██▍       | 144/600 [2:25:26<7:39:57, 60.52s/it] 24%|██▍       | 145/600 [2:26:26<7:38:54, 60.52s/it] 24%|██▍       | 146/600 [2:27:27<7:37:52, 60.51s/it] 24%|██▍       | 147/600 [2:28:27<7:36:48, 60.51s/it] 25%|██▍       | 148/600 [2:29:28<7:35:53, 60.52s/it] 25%|██▍       | 149/600 [2:30:28<7:34:44, 60.50s/it] 25%|██▌       | 150/600 [2:31:29<7:33:44, 60.50s/it]                                                      25%|██▌       | 150/600 [2:31:29<7:33:44, 60.50s/it] 25%|██▌       | 151/600 [2:32:29<7:32:36, 60.48s/it] 25%|██▌       | 152/600 [2:33:30<7:31:43, 60.50s/it] 26%|██▌       | 153/600 [2:34:30<7:30:36, 60.48s/it] 26%|██▌       | 154/600 [2:35:31<7:29:33, 60.48s/it] 26%|██▌       | 155/600 [2:36:31<7:28:33, 60.48s/it] 26%|██▌       | 156/600 [2:37:32<7:27:36, 60.49s/it] 26%|██▌       | 157/600 [2:38:32<7:26:43, 60.50s/it] 26%|██▋       | 158/600 [2:39:33<7:25:37, 60.49s/it] 26%|██▋       | 159/600 [2:40:33<7:24:40, 60.50s/it] 27%|██▋       | 160/600 [2:41:34<7:23:46, 60.51s/it]                                                      27%|██▋       | 160/600 [2:41:34<7:23:46, 60.51s/it] 27%|██▋       | 161/600 [2:42:34<7:22:37, 60.50s/it] 27%|██▋       | 162/600 [2:43:35<7:21:39, 60.50s/it] 27%|██▋       | 163/600 [2:44:35<7:20:35, 60.49s/it] 27%|██▋       | 164/600 [2:45:36<7:19:28, 60.48s/it] 28%|██▊       | 165/600 [2:46:36<7:18:37, 60.50s/it] 28%|██▊       | 166/600 [2:47:37<7:17:37, 60.50s/it] 28%|██▊       | 167/600 [2:48:37<7:16:41, 60.51s/it] 28%|██▊       | 168/600 [2:49:38<7:15:41, 60.51s/it] 28%|██▊       | 169/600 [2:50:38<7:14:43, 60.52s/it] 28%|██▊       | 170/600 [2:51:39<7:13:43, 60.52s/it]                                                      28%|██▊       | 170/600 [2:51:39<7:13:43, 60.52s/it] 28%|██▊       | 171/600 [2:52:39<7:12:45, 60.52s/it] 29%|██▊       | 172/600 [2:53:40<7:11:40, 60.52s/it] 29%|██▉       | 173/600 [2:54:40<7:10:35, 60.50s/it] 29%|██▉       | 174/600 [2:55:41<7:09:37, 60.51s/it] 29%|██▉       | 175/600 [2:56:41<7:08:40, 60.52s/it] 29%|██▉       | 176/600 [2:57:42<7:07:44, 60.53s/it] 30%|██▉       | 177/600 [2:58:43<7:06:43, 60.53s/it] 30%|██▉       | 178/600 [2:59:43<7:05:38, 60.52s/it] 30%|██▉       | 179/600 [3:00:44<7:04:37, 60.52s/it] 30%|███       | 180/600 [3:01:44<7:03:41, 60.53s/it]                                                      30%|███       | 180/600 [3:01:44<7:03:41, 60.53s/it] 30%|███       | 181/600 [3:02:45<7:02:38, 60.52s/it] 30%|███       | 182/600 [3:03:45<7:01:41, 60.53s/it] 30%|███       | 183/600 [3:04:46<7:00:38, 60.52s/it] 31%|███       | 184/600 [3:05:46<6:59:36, 60.52s/it] 31%|███       | 185/600 [3:06:47<6:58:29, 60.50s/it] 31%|███       | 186/600 [3:07:47<6:57:29, 60.51s/it] 31%|███       | 187/600 [3:08:48<6:56:28, 60.50s/it] 31%|███▏      | 188/600 [3:09:48<6:55:25, 60.50s/it] 32%|███▏      | 189/600 [3:10:49<6:54:23, 60.50s/it] 32%|███▏      | 190/600 [3:11:49<6:53:22, 60.49s/it]                                                      32%|███▏      | 190/600 [3:11:49<6:53:22, 60.49s/it] 32%|███▏      | 191/600 [3:12:50<6:52:25, 60.50s/it] 32%|███▏      | 192/600 [3:13:50<6:51:22, 60.50s/it] 32%|███▏      | 193/600 [3:14:51<6:50:21, 60.49s/it] 32%|███▏      | 194/600 [3:15:51<6:49:18, 60.49s/it] 32%|███▎      | 195/600 [3:16:52<6:48:20, 60.49s/it] 33%|███▎      | 196/600 [3:17:52<6:47:16, 60.49s/it] 33%|███▎      | 197/600 [3:18:53<6:46:13, 60.48s/it] 33%|███▎      | 198/600 [3:19:53<6:45:14, 60.48s/it] 33%|███▎      | 199/600 [3:20:53<6:44:10, 60.48s/it] 33%|███▎      | 200/600 [3:21:54<6:43:06, 60.47s/it]                                                      33%|███▎      | 200/600 [3:21:54<6:43:06, 60.47s/it][INFO|configuration_utils.py:458] 2024-05-16 17:17:49,158 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-200/config.json
[INFO|configuration_utils.py:364] 2024-05-16 17:17:49,159 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-200/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-16 17:17:49,165 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-200/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-16 17:17:49,165 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-16 17:17:49,165 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-200/special_tokens_map.json
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 34%|███▎      | 201/600 [3:22:54<6:42:05, 60.47s/it] 34%|███▎      | 202/600 [3:23:55<6:41:08, 60.47s/it] 34%|███▍      | 203/600 [3:24:55<6:40:08, 60.47s/it] 34%|███▍      | 204/600 [3:25:56<6:39:02, 60.46s/it] 34%|███▍      | 205/600 [3:26:56<6:38:01, 60.46s/it] 34%|███▍      | 206/600 [3:27:57<6:37:04, 60.47s/it] 34%|███▍      | 207/600 [3:28:57<6:36:00, 60.46s/it] 35%|███▍      | 208/600 [3:29:58<6:35:01, 60.46s/it] 35%|███▍      | 209/600 [3:30:58<6:33:58, 60.46s/it] 35%|███▌      | 210/600 [3:31:58<6:32:50, 60.44s/it]                                                      35%|███▌      | 210/600 [3:31:58<6:32:50, 60.44s/it] 35%|███▌      | 211/600 [3:32:59<6:31:46, 60.43s/it] 35%|███▌      | 212/600 [3:33:59<6:30:45, 60.43s/it] 36%|███▌      | 213/600 [3:35:00<6:29:46, 60.43s/it] 36%|███▌      | 214/600 [3:36:00<6:28:51, 60.44s/it] 36%|███▌      | 215/600 [3:37:01<6:27:50, 60.44s/it] 36%|███▌      | 216/600 [3:38:01<6:26:50, 60.44s/it] 36%|███▌      | 217/600 [3:39:02<6:25:59, 60.47s/it] 36%|███▋      | 218/600 [3:40:02<6:25:01, 60.48s/it] 36%|███▋      | 219/600 [3:41:03<6:24:04, 60.48s/it] 37%|███▋      | 220/600 [3:42:03<6:23:01, 60.48s/it]                                                      37%|███▋      | 220/600 [3:42:03<6:23:01, 60.48s/it] 37%|███▋      | 221/600 [3:43:04<6:22:01, 60.48s/it] 37%|███▋      | 222/600 [3:44:04<6:20:57, 60.47s/it] 37%|███▋      | 223/600 [3:45:05<6:19:58, 60.47s/it] 37%|███▋      | 224/600 [3:46:05<6:18:58, 60.47s/it] 38%|███▊      | 225/600 [3:47:05<6:17:57, 60.47s/it] 38%|███▊      | 226/600 [3:48:06<6:17:02, 60.49s/it] 38%|███▊      | 227/600 [3:49:07<6:16:11, 60.51s/it] 38%|███▊      | 228/600 [3:50:07<6:15:06, 60.50s/it] 38%|███▊      | 229/600 [3:51:07<6:13:59, 60.48s/it] 38%|███▊      | 230/600 [3:52:08<6:12:56, 60.48s/it]                                                      38%|███▊      | 230/600 [3:52:08<6:12:56, 60.48s/it] 38%|███▊      | 231/600 [3:53:08<6:11:53, 60.47s/it] 39%|███▊      | 232/600 [3:54:09<6:10:56, 60.48s/it] 39%|███▉      | 233/600 [3:55:09<6:09:51, 60.47s/it] 39%|███▉      | 234/600 [3:56:10<6:08:48, 60.46s/it] 39%|███▉      | 235/600 [3:57:10<6:07:46, 60.46s/it] 39%|███▉      | 236/600 [3:58:11<6:06:45, 60.45s/it] 40%|███▉      | 237/600 [3:59:11<6:05:46, 60.46s/it] 40%|███▉      | 238/600 [4:00:12<6:04:41, 60.45s/it] 40%|███▉      | 239/600 [4:01:12<6:03:40, 60.45s/it] 40%|████      | 240/600 [4:02:12<6:02:42, 60.45s/it]                                                      40%|████      | 240/600 [4:02:12<6:02:42, 60.45s/it] 40%|████      | 241/600 [4:03:13<6:01:47, 60.47s/it] 40%|████      | 242/600 [4:04:13<6:00:46, 60.47s/it] 40%|████      | 243/600 [4:05:14<5:59:47, 60.47s/it] 41%|████      | 244/600 [4:06:14<5:58:51, 60.48s/it] 41%|████      | 245/600 [4:07:15<5:57:49, 60.48s/it] 41%|████      | 246/600 [4:08:15<5:56:47, 60.47s/it] 41%|████      | 247/600 [4:09:16<5:55:40, 60.45s/it] 41%|████▏     | 248/600 [4:10:16<5:54:40, 60.46s/it] 42%|████▏     | 249/600 [4:11:17<5:53:38, 60.45s/it] 42%|████▏     | 250/600 [4:12:17<5:52:36, 60.45s/it]                                                      42%|████▏     | 250/600 [4:12:17<5:52:36, 60.45s/it] 42%|████▏     | 251/600 [4:13:18<5:51:33, 60.44s/it] 42%|████▏     | 252/600 [4:14:18<5:50:26, 60.42s/it] 42%|████▏     | 253/600 [4:15:18<5:49:29, 60.43s/it] 42%|████▏     | 254/600 [4:16:19<5:48:25, 60.42s/it] 42%|████▎     | 255/600 [4:17:19<5:47:19, 60.40s/it] 43%|████▎     | 256/600 [4:18:20<5:46:24, 60.42s/it] 43%|████▎     | 257/600 [4:19:20<5:45:22, 60.42s/it] 43%|████▎     | 258/600 [4:20:20<5:44:20, 60.41s/it] 43%|████▎     | 259/600 [4:21:21<5:43:17, 60.40s/it] 43%|████▎     | 260/600 [4:22:21<5:42:23, 60.42s/it]                                                      43%|████▎     | 260/600 [4:22:21<5:42:23, 60.42s/it] 44%|████▎     | 261/600 [4:23:22<5:41:27, 60.44s/it] 44%|████▎     | 262/600 [4:24:22<5:40:29, 60.44s/it] 44%|████▍     | 263/600 [4:25:23<5:39:28, 60.44s/it] 44%|████▍     | 264/600 [4:26:23<5:38:28, 60.44s/it] 44%|████▍     | 265/600 [4:27:23<5:37:26, 60.44s/it] 44%|████▍     | 266/600 [4:28:24<5:36:29, 60.45s/it] 44%|████▍     | 267/600 [4:29:24<5:35:29, 60.45s/it] 45%|████▍     | 268/600 [4:30:25<5:34:24, 60.43s/it] 45%|████▍     | 269/600 [4:31:25<5:33:24, 60.44s/it] 45%|████▌     | 270/600 [4:32:26<5:32:22, 60.43s/it]                                                      45%|████▌     | 270/600 [4:32:26<5:32:22, 60.43s/it] 45%|████▌     | 271/600 [4:33:26<5:31:20, 60.43s/it] 45%|████▌     | 272/600 [4:34:26<5:30:18, 60.42s/it] 46%|████▌     | 273/600 [4:35:27<5:29:23, 60.44s/it] 46%|████▌     | 274/600 [4:36:27<5:28:25, 60.45s/it] 46%|████▌     | 275/600 [4:37:28<5:27:23, 60.44s/it] 46%|████▌     | 276/600 [4:38:28<5:26:23, 60.44s/it] 46%|████▌     | 277/600 [4:39:29<5:25:20, 60.44s/it] 46%|████▋     | 278/600 [4:40:29<5:24:18, 60.43s/it] 46%|████▋     | 279/600 [4:41:30<5:23:16, 60.43s/it] 47%|████▋     | 280/600 [4:42:30<5:22:17, 60.43s/it]                                                      47%|████▋     | 280/600 [4:42:30<5:22:17, 60.43s/it] 47%|████▋     | 281/600 [4:43:30<5:21:19, 60.44s/it] 47%|████▋     | 282/600 [4:44:31<5:20:13, 60.42s/it] 47%|████▋     | 283/600 [4:45:31<5:19:17, 60.44s/it] 47%|████▋     | 284/600 [4:46:32<5:18:18, 60.44s/it] 48%|████▊     | 285/600 [4:47:32<5:17:10, 60.41s/it] 48%|████▊     | 286/600 [4:48:32<5:16:09, 60.41s/it] 48%|████▊     | 287/600 [4:49:33<5:15:15, 60.43s/it] 48%|████▊     | 288/600 [4:50:33<5:14:14, 60.43s/it] 48%|████▊     | 289/600 [4:51:34<5:13:12, 60.43s/it] 48%|████▊     | 290/600 [4:52:34<5:12:11, 60.43s/it]                                                      48%|████▊     | 290/600 [4:52:34<5:12:11, 60.43s/it] 48%|████▊     | 291/600 [4:53:35<5:11:09, 60.42s/it] 49%|████▊     | 292/600 [4:54:35<5:10:06, 60.41s/it] 49%|████▉     | 293/600 [4:55:35<5:08:57, 60.38s/it] 49%|████▉     | 294/600 [4:56:36<5:07:59, 60.39s/it] 49%|████▉     | 295/600 [4:57:36<5:07:06, 60.42s/it] 49%|████▉     | 296/600 [4:58:37<5:06:05, 60.41s/it] 50%|████▉     | 297/600 [4:59:37<5:05:09, 60.43s/it] 50%|████▉     | 298/600 [5:00:38<5:04:09, 60.43s/it] 50%|████▉     | 299/600 [5:01:38<5:03:08, 60.43s/it] 50%|█████     | 300/600 [5:02:38<5:02:05, 60.42s/it]                                                      50%|█████     | 300/600 [5:02:38<5:02:05, 60.42s/it][INFO|configuration_utils.py:458] 2024-05-16 18:58:33,569 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-300/config.json
[INFO|configuration_utils.py:364] 2024-05-16 18:58:33,570 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-300/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-16 18:58:33,576 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-300/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-16 18:58:33,576 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-16 18:58:33,576 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-300/special_tokens_map.json
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 50%|█████     | 301/600 [5:03:39<5:01:10, 60.44s/it] 50%|█████     | 302/600 [5:04:39<5:00:11, 60.44s/it] 50%|█████     | 303/600 [5:05:40<4:59:07, 60.43s/it] 51%|█████     | 304/600 [5:06:40<4:58:11, 60.44s/it] 51%|█████     | 305/600 [5:07:41<4:57:05, 60.42s/it] 51%|█████     | 306/600 [5:08:41<4:56:04, 60.42s/it] 51%|█████     | 307/600 [5:09:41<4:55:02, 60.42s/it] 51%|█████▏    | 308/600 [5:10:42<4:54:11, 60.45s/it] 52%|█████▏    | 309/600 [5:11:42<4:53:12, 60.45s/it] 52%|█████▏    | 310/600 [5:12:43<4:52:09, 60.44s/it]                                                      52%|█████▏    | 310/600 [5:12:43<4:52:09, 60.44s/it] 52%|█████▏    | 311/600 [5:13:43<4:51:07, 60.44s/it] 52%|█████▏    | 312/600 [5:14:44<4:50:05, 60.44s/it] 52%|█████▏    | 313/600 [5:15:44<4:49:04, 60.43s/it] 52%|█████▏    | 314/600 [5:16:44<4:48:01, 60.42s/it] 52%|█████▎    | 315/600 [5:17:45<4:47:02, 60.43s/it] 53%|█████▎    | 316/600 [5:18:45<4:46:03, 60.44s/it] 53%|█████▎    | 317/600 [5:19:46<4:45:00, 60.42s/it] 53%|█████▎    | 318/600 [5:20:46<4:44:01, 60.43s/it] 53%|█████▎    | 319/600 [5:21:47<4:43:05, 60.45s/it] 53%|█████▎    | 320/600 [5:22:47<4:42:02, 60.44s/it]                                                      53%|█████▎    | 320/600 [5:22:47<4:42:02, 60.44s/it] 54%|█████▎    | 321/600 [5:23:48<4:41:01, 60.44s/it] 54%|█████▎    | 322/600 [5:24:48<4:39:58, 60.43s/it] 54%|█████▍    | 323/600 [5:25:48<4:38:54, 60.41s/it] 54%|█████▍    | 324/600 [5:26:49<4:37:54, 60.42s/it] 54%|█████▍    | 325/600 [5:27:49<4:36:55, 60.42s/it] 54%|█████▍    | 326/600 [5:28:50<4:35:57, 60.43s/it] 55%|█████▍    | 327/600 [5:29:50<4:34:52, 60.41s/it] 55%|█████▍    | 328/600 [5:30:50<4:33:54, 60.42s/it] 55%|█████▍    | 329/600 [5:31:51<4:32:51, 60.41s/it] 55%|█████▌    | 330/600 [5:32:51<4:31:51, 60.41s/it]                                                      55%|█████▌    | 330/600 [5:32:51<4:31:51, 60.41s/it] 55%|█████▌    | 331/600 [5:33:52<4:30:47, 60.40s/it] 55%|█████▌    | 332/600 [5:34:52<4:29:50, 60.41s/it] 56%|█████▌    | 333/600 [5:35:52<4:28:47, 60.40s/it] 56%|█████▌    | 334/600 [5:36:53<4:27:48, 60.41s/it] 56%|█████▌    | 335/600 [5:37:53<4:26:46, 60.40s/it] 56%|█████▌    | 336/600 [5:38:54<4:25:43, 60.39s/it] 56%|█████▌    | 337/600 [5:39:54<4:24:44, 60.40s/it] 56%|█████▋    | 338/600 [5:40:54<4:23:44, 60.40s/it] 56%|█████▋    | 339/600 [5:41:55<4:22:39, 60.38s/it] 57%|█████▋    | 340/600 [5:42:55<4:21:37, 60.38s/it]                                                      57%|█████▋    | 340/600 [5:42:55<4:21:37, 60.38s/it] 57%|█████▋    | 341/600 [5:43:56<4:20:39, 60.38s/it] 57%|█████▋    | 342/600 [5:44:56<4:19:42, 60.40s/it] 57%|█████▋    | 343/600 [5:45:56<4:18:43, 60.40s/it] 57%|█████▋    | 344/600 [5:46:57<4:17:44, 60.41s/it] 57%|█████▊    | 345/600 [5:47:57<4:16:45, 60.41s/it] 58%|█████▊    | 346/600 [5:48:58<4:15:43, 60.41s/it] 58%|█████▊    | 347/600 [5:49:58<4:14:45, 60.42s/it] 58%|█████▊    | 348/600 [5:50:58<4:13:43, 60.41s/it] 58%|█████▊    | 349/600 [5:51:59<4:12:40, 60.40s/it] 58%|█████▊    | 350/600 [5:52:59<4:11:34, 60.38s/it]                                                      58%|█████▊    | 350/600 [5:52:59<4:11:34, 60.38s/it] 58%|█████▊    | 351/600 [5:54:00<4:10:36, 60.39s/it] 59%|█████▊    | 352/600 [5:55:00<4:09:36, 60.39s/it] 59%|█████▉    | 353/600 [5:56:00<4:08:36, 60.39s/it] 59%|█████▉    | 354/600 [5:57:01<4:07:38, 60.40s/it] 59%|█████▉    | 355/600 [5:58:01<4:06:39, 60.41s/it] 59%|█████▉    | 356/600 [5:59:02<4:05:39, 60.41s/it] 60%|█████▉    | 357/600 [6:00:02<4:04:45, 60.43s/it] 60%|█████▉    | 358/600 [6:01:03<4:03:44, 60.43s/it] 60%|█████▉    | 359/600 [6:02:03<4:02:41, 60.42s/it] 60%|██████    | 360/600 [6:03:03<4:01:38, 60.41s/it]                                                      60%|██████    | 360/600 [6:03:03<4:01:38, 60.41s/it] 60%|██████    | 361/600 [6:04:04<4:00:36, 60.41s/it] 60%|██████    | 362/600 [6:05:04<3:59:36, 60.40s/it] 60%|██████    | 363/600 [6:06:04<3:58:34, 60.40s/it] 61%|██████    | 364/600 [6:07:05<3:57:36, 60.41s/it] 61%|██████    | 365/600 [6:08:05<3:56:34, 60.40s/it] 61%|██████    | 366/600 [6:09:06<3:55:37, 60.42s/it] 61%|██████    | 367/600 [6:10:06<3:54:31, 60.39s/it] 61%|██████▏   | 368/600 [6:11:06<3:53:31, 60.39s/it] 62%|██████▏   | 369/600 [6:12:07<3:52:28, 60.39s/it] 62%|██████▏   | 370/600 [6:13:07<3:51:28, 60.39s/it]                                                      62%|██████▏   | 370/600 [6:13:07<3:51:28, 60.39s/it] 62%|██████▏   | 371/600 [6:14:08<3:50:27, 60.38s/it] 62%|██████▏   | 372/600 [6:15:08<3:49:29, 60.39s/it] 62%|██████▏   | 373/600 [6:16:08<3:48:26, 60.38s/it] 62%|██████▏   | 374/600 [6:17:09<3:47:28, 60.39s/it] 62%|██████▎   | 375/600 [6:18:09<3:46:24, 60.38s/it] 63%|██████▎   | 376/600 [6:19:09<3:45:22, 60.37s/it] 63%|██████▎   | 377/600 [6:20:10<3:44:21, 60.37s/it] 63%|██████▎   | 378/600 [6:21:10<3:43:24, 60.38s/it] 63%|██████▎   | 379/600 [6:22:11<3:42:20, 60.37s/it] 63%|██████▎   | 380/600 [6:23:11<3:41:23, 60.38s/it]                                                      63%|██████▎   | 380/600 [6:23:11<3:41:23, 60.38s/it] 64%|██████▎   | 381/600 [6:24:11<3:40:21, 60.37s/it] 64%|██████▎   | 382/600 [6:25:12<3:39:28, 60.40s/it] 64%|██████▍   | 383/600 [6:26:12<3:38:29, 60.41s/it] 64%|██████▍   | 384/600 [6:27:13<3:37:33, 60.43s/it] 64%|██████▍   | 385/600 [6:28:13<3:36:30, 60.42s/it] 64%|██████▍   | 386/600 [6:29:14<3:35:29, 60.42s/it] 64%|██████▍   | 387/600 [6:30:14<3:34:34, 60.44s/it] 65%|██████▍   | 388/600 [6:31:14<3:33:28, 60.42s/it] 65%|██████▍   | 389/600 [6:32:15<3:32:25, 60.41s/it] 65%|██████▌   | 390/600 [6:33:15<3:31:25, 60.41s/it]                                                      65%|██████▌   | 390/600 [6:33:15<3:31:25, 60.41s/it] 65%|██████▌   | 391/600 [6:34:16<3:30:26, 60.42s/it] 65%|██████▌   | 392/600 [6:35:16<3:29:24, 60.41s/it] 66%|██████▌   | 393/600 [6:36:16<3:28:22, 60.40s/it] 66%|██████▌   | 394/600 [6:37:17<3:27:21, 60.40s/it] 66%|██████▌   | 395/600 [6:38:17<3:26:19, 60.39s/it] 66%|██████▌   | 396/600 [6:39:18<3:25:21, 60.40s/it] 66%|██████▌   | 397/600 [6:40:18<3:24:22, 60.41s/it] 66%|██████▋   | 398/600 [6:41:18<3:23:26, 60.43s/it] 66%|██████▋   | 399/600 [6:42:19<3:22:31, 60.45s/it] 67%|██████▋   | 400/600 [6:43:19<3:21:30, 60.45s/it]                                                      67%|██████▋   | 400/600 [6:43:19<3:21:30, 60.45s/it][INFO|configuration_utils.py:458] 2024-05-16 20:39:14,676 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-400/config.json
[INFO|configuration_utils.py:364] 2024-05-16 20:39:14,677 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-400/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-16 20:39:14,683 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-400/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-16 20:39:14,683 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-16 20:39:14,683 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-400/special_tokens_map.json
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 67%|██████▋   | 401/600 [6:44:20<3:20:31, 60.46s/it] 67%|██████▋   | 402/600 [6:45:20<3:19:26, 60.44s/it] 67%|██████▋   | 403/600 [6:46:21<3:18:29, 60.46s/it] 67%|██████▋   | 404/600 [6:47:21<3:17:30, 60.46s/it] 68%|██████▊   | 405/600 [6:48:22<3:16:28, 60.45s/it] 68%|██████▊   | 406/600 [6:49:22<3:15:24, 60.44s/it] 68%|██████▊   | 407/600 [6:50:22<3:14:20, 60.42s/it] 68%|██████▊   | 408/600 [6:51:23<3:13:17, 60.40s/it] 68%|██████▊   | 409/600 [6:52:23<3:12:18, 60.41s/it] 68%|██████▊   | 410/600 [6:53:24<3:11:20, 60.42s/it]                                                      68%|██████▊   | 410/600 [6:53:24<3:11:20, 60.42s/it] 68%|██████▊   | 411/600 [6:54:24<3:10:17, 60.41s/it] 69%|██████▊   | 412/600 [6:55:24<3:09:16, 60.40s/it] 69%|██████▉   | 413/600 [6:56:25<3:08:19, 60.42s/it] 69%|██████▉   | 414/600 [6:57:25<3:07:17, 60.41s/it] 69%|██████▉   | 415/600 [6:58:26<3:06:18, 60.43s/it] 69%|██████▉   | 416/600 [6:59:26<3:05:15, 60.41s/it] 70%|██████▉   | 417/600 [7:00:27<3:04:17, 60.42s/it] 70%|██████▉   | 418/600 [7:01:27<3:03:20, 60.44s/it] 70%|██████▉   | 419/600 [7:02:28<3:02:20, 60.44s/it] 70%|███████   | 420/600 [7:03:28<3:01:20, 60.45s/it]                                                      70%|███████   | 420/600 [7:03:28<3:01:20, 60.45s/it] 70%|███████   | 421/600 [7:04:28<3:00:16, 60.43s/it] 70%|███████   | 422/600 [7:05:29<2:59:15, 60.43s/it] 70%|███████   | 423/600 [7:06:29<2:58:11, 60.40s/it] 71%|███████   | 424/600 [7:07:30<2:57:12, 60.41s/it] 71%|███████   | 425/600 [7:08:30<2:56:08, 60.39s/it] 71%|███████   | 426/600 [7:09:30<2:55:07, 60.39s/it] 71%|███████   | 427/600 [7:10:31<2:54:08, 60.40s/it] 71%|███████▏  | 428/600 [7:11:31<2:53:06, 60.38s/it] 72%|███████▏  | 429/600 [7:12:32<2:52:08, 60.40s/it] 72%|███████▏  | 430/600 [7:13:32<2:51:06, 60.39s/it]                                                      72%|███████▏  | 430/600 [7:13:32<2:51:06, 60.39s/it] 72%|███████▏  | 431/600 [7:14:32<2:50:04, 60.38s/it] 72%|███████▏  | 432/600 [7:15:33<2:49:05, 60.39s/it] 72%|███████▏  | 433/600 [7:16:33<2:48:06, 60.40s/it] 72%|███████▏  | 434/600 [7:17:34<2:47:06, 60.40s/it] 72%|███████▎  | 435/600 [7:18:34<2:46:05, 60.40s/it] 73%|███████▎  | 436/600 [7:19:34<2:45:03, 60.39s/it] 73%|███████▎  | 437/600 [7:20:35<2:44:03, 60.39s/it] 73%|███████▎  | 438/600 [7:21:35<2:43:04, 60.40s/it] 73%|███████▎  | 439/600 [7:22:35<2:42:05, 60.40s/it] 73%|███████▎  | 440/600 [7:23:36<2:41:05, 60.41s/it]                                                      73%|███████▎  | 440/600 [7:23:36<2:41:05, 60.41s/it] 74%|███████▎  | 441/600 [7:24:36<2:40:05, 60.41s/it] 74%|███████▎  | 442/600 [7:25:37<2:39:05, 60.41s/it] 74%|███████▍  | 443/600 [7:26:37<2:38:03, 60.41s/it] 74%|███████▍  | 444/600 [7:27:37<2:37:00, 60.39s/it] 74%|███████▍  | 445/600 [7:28:38<2:35:56, 60.36s/it] 74%|███████▍  | 446/600 [7:29:38<2:34:56, 60.36s/it] 74%|███████▍  | 447/600 [7:30:39<2:33:55, 60.36s/it] 75%|███████▍  | 448/600 [7:31:39<2:32:56, 60.37s/it] 75%|███████▍  | 449/600 [7:32:39<2:31:57, 60.38s/it] 75%|███████▌  | 450/600 [7:33:40<2:30:58, 60.39s/it]                                                      75%|███████▌  | 450/600 [7:33:40<2:30:58, 60.39s/it] 75%|███████▌  | 451/600 [7:34:40<2:29:58, 60.39s/it] 75%|███████▌  | 452/600 [7:35:41<2:29:00, 60.41s/it] 76%|███████▌  | 453/600 [7:36:41<2:28:00, 60.41s/it] 76%|███████▌  | 454/600 [7:37:41<2:27:01, 60.42s/it] 76%|███████▌  | 455/600 [7:38:42<2:26:00, 60.42s/it] 76%|███████▌  | 456/600 [7:39:42<2:24:56, 60.39s/it] 76%|███████▌  | 457/600 [7:40:43<2:23:55, 60.39s/it] 76%|███████▋  | 458/600 [7:41:43<2:22:56, 60.40s/it] 76%|███████▋  | 459/600 [7:42:43<2:21:54, 60.39s/it] 77%|███████▋  | 460/600 [7:43:44<2:20:53, 60.38s/it]                                                      77%|███████▋  | 460/600 [7:43:44<2:20:53, 60.38s/it] 77%|███████▋  | 461/600 [7:44:44<2:19:56, 60.41s/it] 77%|███████▋  | 462/600 [7:45:45<2:18:55, 60.40s/it] 77%|███████▋  | 463/600 [7:46:45<2:17:54, 60.40s/it] 77%|███████▋  | 464/600 [7:47:45<2:16:53, 60.39s/it] 78%|███████▊  | 465/600 [7:48:46<2:15:54, 60.40s/it] 78%|███████▊  | 466/600 [7:49:46<2:14:54, 60.41s/it] 78%|███████▊  | 467/600 [7:50:47<2:13:54, 60.41s/it] 78%|███████▊  | 468/600 [7:51:47<2:12:55, 60.42s/it] 78%|███████▊  | 469/600 [7:52:47<2:11:55, 60.42s/it] 78%|███████▊  | 470/600 [7:53:48<2:10:54, 60.42s/it]                                                      78%|███████▊  | 470/600 [7:53:48<2:10:54, 60.42s/it] 78%|███████▊  | 471/600 [7:54:48<2:09:53, 60.42s/it] 79%|███████▊  | 472/600 [7:55:49<2:08:53, 60.42s/it] 79%|███████▉  | 473/600 [7:56:49<2:07:51, 60.40s/it] 79%|███████▉  | 474/600 [7:57:49<2:06:50, 60.40s/it] 79%|███████▉  | 475/600 [7:58:50<2:05:49, 60.40s/it] 79%|███████▉  | 476/600 [7:59:50<2:04:50, 60.41s/it] 80%|███████▉  | 477/600 [8:00:51<2:03:51, 60.42s/it] 80%|███████▉  | 478/600 [8:01:51<2:02:51, 60.42s/it] 80%|███████▉  | 479/600 [8:02:52<2:01:50, 60.41s/it] 80%|████████  | 480/600 [8:03:52<2:00:49, 60.42s/it]                                                      80%|████████  | 480/600 [8:03:52<2:00:49, 60.42s/it] 80%|████████  | 481/600 [8:04:52<1:59:48, 60.41s/it] 80%|████████  | 482/600 [8:05:53<1:58:49, 60.42s/it] 80%|████████  | 483/600 [8:06:53<1:57:48, 60.42s/it] 81%|████████  | 484/600 [8:07:54<1:56:48, 60.42s/it] 81%|████████  | 485/600 [8:08:54<1:55:49, 60.43s/it] 81%|████████  | 486/600 [8:09:54<1:54:47, 60.41s/it] 81%|████████  | 487/600 [8:10:55<1:53:45, 60.40s/it] 81%|████████▏ | 488/600 [8:11:55<1:52:43, 60.39s/it] 82%|████████▏ | 489/600 [8:12:56<1:51:43, 60.39s/it] 82%|████████▏ | 490/600 [8:13:56<1:50:44, 60.41s/it]                                                      82%|████████▏ | 490/600 [8:13:56<1:50:44, 60.41s/it] 82%|████████▏ | 491/600 [8:14:57<1:49:46, 60.43s/it] 82%|████████▏ | 492/600 [8:15:57<1:48:46, 60.43s/it] 82%|████████▏ | 493/600 [8:16:57<1:47:44, 60.41s/it] 82%|████████▏ | 494/600 [8:17:58<1:46:42, 60.40s/it] 82%|████████▎ | 495/600 [8:18:58<1:45:41, 60.39s/it] 83%|████████▎ | 496/600 [8:19:58<1:44:40, 60.39s/it] 83%|████████▎ | 497/600 [8:20:59<1:43:39, 60.39s/it] 83%|████████▎ | 498/600 [8:21:59<1:42:40, 60.39s/it] 83%|████████▎ | 499/600 [8:23:00<1:41:40, 60.40s/it] 83%|████████▎ | 500/600 [8:24:00<1:40:42, 60.42s/it]                                                      83%|████████▎ | 500/600 [8:24:00<1:40:42, 60.42s/it][INFO|configuration_utils.py:458] 2024-05-16 22:19:55,359 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-500/config.json
[INFO|configuration_utils.py:364] 2024-05-16 22:19:55,359 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-16 22:19:55,365 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-500/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-16 22:19:55,366 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-16 22:19:55,366 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-500/special_tokens_map.json
/udata/anaconda3/envs/lorax/lib/python3.9/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
 84%|████████▎ | 501/600 [8:25:01<1:39:41, 60.42s/it] 84%|████████▎ | 502/600 [8:26:01<1:38:43, 60.44s/it] 84%|████████▍ | 503/600 [8:27:01<1:37:42, 60.43s/it] 84%|████████▍ | 504/600 [8:28:02<1:36:42, 60.44s/it] 84%|████████▍ | 505/600 [8:29:02<1:35:41, 60.43s/it] 84%|████████▍ | 506/600 [8:30:03<1:34:38, 60.41s/it] 84%|████████▍ | 507/600 [8:31:03<1:33:37, 60.41s/it] 85%|████████▍ | 508/600 [8:32:04<1:32:37, 60.41s/it] 85%|████████▍ | 509/600 [8:33:04<1:31:37, 60.42s/it] 85%|████████▌ | 510/600 [8:34:04<1:30:36, 60.41s/it]                                                      85%|████████▌ | 510/600 [8:34:04<1:30:36, 60.41s/it] 85%|████████▌ | 511/600 [8:35:05<1:29:36, 60.41s/it] 85%|████████▌ | 512/600 [8:36:05<1:28:37, 60.43s/it] 86%|████████▌ | 513/600 [8:37:06<1:27:37, 60.43s/it] 86%|████████▌ | 514/600 [8:38:06<1:26:35, 60.41s/it] 86%|████████▌ | 515/600 [8:39:06<1:25:33, 60.39s/it] 86%|████████▌ | 516/600 [8:40:07<1:24:33, 60.40s/it] 86%|████████▌ | 517/600 [8:41:07<1:23:33, 60.40s/it] 86%|████████▋ | 518/600 [8:42:08<1:22:32, 60.39s/it] 86%|████████▋ | 519/600 [8:43:08<1:21:32, 60.40s/it] 87%|████████▋ | 520/600 [8:44:08<1:20:30, 60.38s/it]                                                      87%|████████▋ | 520/600 [8:44:08<1:20:30, 60.38s/it] 87%|████████▋ | 521/600 [8:45:09<1:19:29, 60.38s/it] 87%|████████▋ | 522/600 [8:46:09<1:18:31, 60.40s/it] 87%|████████▋ | 523/600 [8:47:10<1:17:31, 60.42s/it] 87%|████████▋ | 524/600 [8:48:10<1:16:31, 60.41s/it] 88%|████████▊ | 525/600 [8:49:10<1:15:32, 60.43s/it] 88%|████████▊ | 526/600 [8:50:11<1:14:30, 60.41s/it] 88%|████████▊ | 527/600 [8:51:11<1:13:29, 60.40s/it] 88%|████████▊ | 528/600 [8:52:12<1:12:29, 60.42s/it] 88%|████████▊ | 529/600 [8:53:12<1:11:28, 60.41s/it] 88%|████████▊ | 530/600 [8:54:12<1:10:28, 60.41s/it]                                                      88%|████████▊ | 530/600 [8:54:12<1:10:28, 60.41s/it] 88%|████████▊ | 531/600 [8:55:13<1:09:27, 60.39s/it] 89%|████████▊ | 532/600 [8:56:13<1:08:27, 60.40s/it] 89%|████████▉ | 533/600 [8:57:14<1:07:28, 60.42s/it] 89%|████████▉ | 534/600 [8:58:14<1:06:27, 60.42s/it] 89%|████████▉ | 535/600 [8:59:15<1:05:28, 60.43s/it] 89%|████████▉ | 536/600 [9:00:15<1:04:26, 60.42s/it] 90%|████████▉ | 537/600 [9:01:15<1:03:26, 60.42s/it] 90%|████████▉ | 538/600 [9:02:16<1:02:26, 60.43s/it] 90%|████████▉ | 539/600 [9:03:16<1:01:25, 60.41s/it] 90%|█████████ | 540/600 [9:04:17<1:00:24, 60.41s/it]                                                      90%|█████████ | 540/600 [9:04:17<1:00:24, 60.41s/it] 90%|█████████ | 541/600 [9:05:17<59:23, 60.40s/it]   90%|█████████ | 542/600 [9:06:17<58:24, 60.42s/it] 90%|█████████ | 543/600 [9:07:18<57:24, 60.43s/it] 91%|█████████ | 544/600 [9:08:18<56:23, 60.41s/it] 91%|█████████ | 545/600 [9:09:19<55:22, 60.41s/it] 91%|█████████ | 546/600 [9:10:19<54:21, 60.40s/it] 91%|█████████ | 547/600 [9:11:19<53:21, 60.40s/it] 91%|█████████▏| 548/600 [9:12:20<52:20, 60.40s/it] 92%|█████████▏| 549/600 [9:13:20<51:20, 60.41s/it] 92%|█████████▏| 550/600 [9:14:21<50:20, 60.40s/it]                                                    92%|█████████▏| 550/600 [9:14:21<50:20, 60.40s/it] 92%|█████████▏| 551/600 [9:15:21<49:19, 60.40s/it] 92%|█████████▏| 552/600 [9:16:21<48:18, 60.39s/it] 92%|█████████▏| 553/600 [9:17:22<47:18, 60.39s/it] 92%|█████████▏| 554/600 [9:18:22<46:17, 60.39s/it] 92%|█████████▎| 555/600 [9:19:23<45:17, 60.38s/it] 93%|█████████▎| 556/600 [9:20:23<44:16, 60.38s/it] 93%|█████████▎| 557/600 [9:21:23<43:16, 60.38s/it] 93%|█████████▎| 558/600 [9:22:24<42:15, 60.38s/it] 93%|█████████▎| 559/600 [9:23:24<41:15, 60.38s/it] 93%|█████████▎| 560/600 [9:24:24<40:14, 60.37s/it]                                                    93%|█████████▎| 560/600 [9:24:24<40:14, 60.37s/it] 94%|█████████▎| 561/600 [9:25:25<39:14, 60.36s/it] 94%|█████████▎| 562/600 [9:26:25<38:13, 60.35s/it] 94%|█████████▍| 563/600 [9:27:25<37:13, 60.35s/it] 94%|█████████▍| 564/600 [9:28:26<36:13, 60.36s/it] 94%|█████████▍| 565/600 [9:29:26<35:12, 60.37s/it] 94%|█████████▍| 566/600 [9:30:27<34:12, 60.36s/it] 94%|█████████▍| 567/600 [9:31:27<33:12, 60.38s/it] 95%|█████████▍| 568/600 [9:32:27<32:12, 60.39s/it] 95%|█████████▍| 569/600 [9:33:28<31:12, 60.40s/it] 95%|█████████▌| 570/600 [9:34:28<30:12, 60.40s/it]                                                    95%|█████████▌| 570/600 [9:34:28<30:12, 60.40s/it] 95%|█████████▌| 571/600 [9:35:29<29:11, 60.39s/it] 95%|█████████▌| 572/600 [9:36:29<28:11, 60.40s/it] 96%|█████████▌| 573/600 [9:37:29<27:10, 60.40s/it] 96%|█████████▌| 574/600 [9:38:30<26:10, 60.39s/it] 96%|█████████▌| 575/600 [9:39:30<25:10, 60.40s/it] 96%|█████████▌| 576/600 [9:40:31<24:09, 60.41s/it] 96%|█████████▌| 577/600 [9:41:31<23:09, 60.42s/it] 96%|█████████▋| 578/600 [9:42:32<22:09, 60.42s/it] 96%|█████████▋| 579/600 [9:43:32<21:08, 60.42s/it] 97%|█████████▋| 580/600 [9:44:32<20:08, 60.40s/it]                                                    97%|█████████▋| 580/600 [9:44:32<20:08, 60.40s/it] 97%|█████████▋| 581/600 [9:45:33<19:07, 60.40s/it] 97%|█████████▋| 582/600 [9:46:33<18:07, 60.41s/it] 97%|█████████▋| 583/600 [9:47:33<17:06, 60.39s/it] 97%|█████████▋| 584/600 [9:48:34<16:06, 60.39s/it] 98%|█████████▊| 585/600 [9:49:34<15:05, 60.39s/it] 98%|█████████▊| 586/600 [9:50:35<14:05, 60.39s/it] 98%|█████████▊| 587/600 [9:51:35<13:05, 60.40s/it] 98%|█████████▊| 588/600 [9:52:35<12:04, 60.40s/it] 98%|█████████▊| 589/600 [9:53:36<11:04, 60.40s/it] 98%|█████████▊| 590/600 [9:54:36<10:04, 60.41s/it]                                                    98%|█████████▊| 590/600 [9:54:36<10:04, 60.41s/it] 98%|█████████▊| 591/600 [9:55:37<09:03, 60.41s/it] 99%|█████████▊| 592/600 [9:56:37<08:03, 60.39s/it] 99%|█████████▉| 593/600 [9:57:37<07:02, 60.39s/it] 99%|█████████▉| 594/600 [9:58:38<06:02, 60.39s/it] 99%|█████████▉| 595/600 [9:59:38<05:01, 60.40s/it] 99%|█████████▉| 596/600 [10:00:39<04:01, 60.40s/it]100%|█████████▉| 597/600 [10:01:39<03:01, 60.39s/it]100%|█████████▉| 598/600 [10:02:39<02:00, 60.39s/it]100%|█████████▉| 599/600 [10:03:40<01:00, 60.39s/it]100%|██████████| 600/600 [10:04:40<00:00, 60.39s/it]                                                    100%|██████████| 600/600 [10:04:40<00:00, 60.39s/it][INFO|configuration_utils.py:458] 2024-05-17 00:00:35,437 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-600/config.json
[INFO|configuration_utils.py:364] 2024-05-17 00:00:35,438 >> Configuration saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-600/generation_config.json
[INFO|modeling_utils.py:1853] 2024-05-17 00:00:35,443 >> Model weights saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-600/pytorch_model.bin
[INFO|tokenization_utils_base.py:2194] 2024-05-17 00:00:35,444 >> tokenizer config file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2201] 2024-05-17 00:00:35,444 >> Special tokens file saved in output/Fin-Train-chatglm2-6b-pt-128-2e-2/checkpoint-600/special_tokens_map.json
[INFO|trainer.py:2053] 2024-05-17 00:00:35,457 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


                                                    100%|██████████| 600/600 [10:04:40<00:00, 60.39s/it]100%|██████████| 600/600 [10:04:40<00:00, 60.47s/it]
{'loss': 0.8327, 'learning_rate': 0.019666666666666666, 'epoch': 0.15}
{'loss': 0.2575, 'learning_rate': 0.019333333333333334, 'epoch': 0.31}
{'loss': 0.1468, 'learning_rate': 0.019, 'epoch': 0.46}
{'loss': 0.1148, 'learning_rate': 0.018666666666666668, 'epoch': 0.61}
{'loss': 0.1044, 'learning_rate': 0.018333333333333333, 'epoch': 0.77}
{'loss': 0.0721, 'learning_rate': 0.018000000000000002, 'epoch': 0.92}
{'loss': 0.0548, 'learning_rate': 0.017666666666666667, 'epoch': 1.08}
{'loss': 0.0445, 'learning_rate': 0.017333333333333336, 'epoch': 1.23}
{'loss': 0.0385, 'learning_rate': 0.017, 'epoch': 1.38}
{'loss': 0.0318, 'learning_rate': 0.016666666666666666, 'epoch': 1.54}
Saving PrefixEncoder
{'loss': 0.0289, 'learning_rate': 0.01633333333333333, 'epoch': 1.69}
{'loss': 0.0347, 'learning_rate': 0.016, 'epoch': 1.84}
{'loss': 0.0262, 'learning_rate': 0.015666666666666666, 'epoch': 2.0}
{'loss': 0.0225, 'learning_rate': 0.015333333333333334, 'epoch': 2.15}
{'loss': 0.0216, 'learning_rate': 0.015, 'epoch': 2.31}
{'loss': 0.0217, 'learning_rate': 0.014666666666666666, 'epoch': 2.46}
{'loss': 0.0209, 'learning_rate': 0.014333333333333333, 'epoch': 2.61}
{'loss': 0.0229, 'learning_rate': 0.013999999999999999, 'epoch': 2.77}
{'loss': 0.0208, 'learning_rate': 0.013666666666666667, 'epoch': 2.92}
{'loss': 0.0167, 'learning_rate': 0.013333333333333332, 'epoch': 3.07}
Saving PrefixEncoder
{'loss': 0.0149, 'learning_rate': 0.013000000000000001, 'epoch': 3.23}
{'loss': 0.0162, 'learning_rate': 0.012666666666666666, 'epoch': 3.38}
{'loss': 0.0168, 'learning_rate': 0.012333333333333335, 'epoch': 3.54}
{'loss': 0.0125, 'learning_rate': 0.012, 'epoch': 3.69}
{'loss': 0.0163, 'learning_rate': 0.011666666666666667, 'epoch': 3.84}
{'loss': 0.011, 'learning_rate': 0.011333333333333332, 'epoch': 4.0}
{'loss': 0.0106, 'learning_rate': 0.011000000000000001, 'epoch': 4.15}
{'loss': 0.009, 'learning_rate': 0.010666666666666666, 'epoch': 4.3}
{'loss': 0.0106, 'learning_rate': 0.010333333333333335, 'epoch': 4.46}
{'loss': 0.0142, 'learning_rate': 0.01, 'epoch': 4.61}
Saving PrefixEncoder
{'loss': 0.0112, 'learning_rate': 0.009666666666666667, 'epoch': 4.76}
{'loss': 0.0124, 'learning_rate': 0.009333333333333334, 'epoch': 4.92}
{'loss': 0.0113, 'learning_rate': 0.009000000000000001, 'epoch': 5.07}
{'loss': 0.0093, 'learning_rate': 0.008666666666666668, 'epoch': 5.23}
{'loss': 0.0089, 'learning_rate': 0.008333333333333333, 'epoch': 5.38}
{'loss': 0.0099, 'learning_rate': 0.008, 'epoch': 5.53}
{'loss': 0.0074, 'learning_rate': 0.007666666666666667, 'epoch': 5.69}
{'loss': 0.0102, 'learning_rate': 0.007333333333333333, 'epoch': 5.84}
{'loss': 0.0084, 'learning_rate': 0.006999999999999999, 'epoch': 5.99}
{'loss': 0.0074, 'learning_rate': 0.006666666666666666, 'epoch': 6.15}
Saving PrefixEncoder
{'loss': 0.0109, 'learning_rate': 0.006333333333333333, 'epoch': 6.3}
{'loss': 0.0098, 'learning_rate': 0.006, 'epoch': 6.46}
{'loss': 0.0085, 'learning_rate': 0.005666666666666666, 'epoch': 6.61}
{'loss': 0.0083, 'learning_rate': 0.005333333333333333, 'epoch': 6.76}
{'loss': 0.0072, 'learning_rate': 0.005, 'epoch': 6.92}
{'loss': 0.0084, 'learning_rate': 0.004666666666666667, 'epoch': 7.07}
{'loss': 0.0067, 'learning_rate': 0.004333333333333334, 'epoch': 7.22}
{'loss': 0.0093, 'learning_rate': 0.004, 'epoch': 7.38}
{'loss': 0.0074, 'learning_rate': 0.0036666666666666666, 'epoch': 7.53}
{'loss': 0.0079, 'learning_rate': 0.003333333333333333, 'epoch': 7.68}
Saving PrefixEncoder
{'loss': 0.0085, 'learning_rate': 0.003, 'epoch': 7.84}
{'loss': 0.0074, 'learning_rate': 0.0026666666666666666, 'epoch': 7.99}
{'loss': 0.0071, 'learning_rate': 0.0023333333333333335, 'epoch': 8.15}
{'loss': 0.0078, 'learning_rate': 0.002, 'epoch': 8.3}
{'loss': 0.0076, 'learning_rate': 0.0016666666666666666, 'epoch': 8.45}
{'loss': 0.0052, 'learning_rate': 0.0013333333333333333, 'epoch': 8.61}
{'loss': 0.0062, 'learning_rate': 0.001, 'epoch': 8.76}
{'loss': 0.0061, 'learning_rate': 0.0006666666666666666, 'epoch': 8.91}
{'loss': 0.0068, 'learning_rate': 0.0003333333333333333, 'epoch': 9.07}
{'loss': 0.007, 'learning_rate': 0.0, 'epoch': 9.22}
Saving PrefixEncoder
{'train_runtime': 36280.7287, 'train_samples_per_second': 0.265, 'train_steps_per_second': 0.017, 'train_loss': 0.03862040589253108, 'epoch': 9.22}
***** train metrics *****
  epoch                    =        9.22
  train_loss               =      0.0386
  train_runtime            = 10:04:40.72
  train_samples            =        1041
  train_samples_per_second =       0.265
  train_steps_per_second   =       0.017
