# 阶段15：项目实战2：大模型AI+：金融对话交互智能系统

### 1、项目背景

本项目要求以大模型为中心制作一个金融对话交互系统，回答用户的金融相关的问题，数据集来源于真实的11588 份 2019 年至 2021 年期间的上市公司年度报告。

项目的目标是深度解析上市公司年报的对话交互智能系统。面对金融文本中的专业术语与暗含信息，致力于用AI实现专家级别的金融分析。在AI领域，虽然已在文本对话取得进展，但真正的金融交互场景仍然是一个巨大挑战。上市公司年报为投资者呈现了公司的经营状况、财务状况和未来规划。专业知识是解读的关键，而项目的目标是通过AI技术让这一过程变得更简单、更准确。落地应用可用于股票交易问答助手，智能金融行业分析助手等。

按照涉及模型的能力和复杂程度大体分为初级、中级、高级三种类型：

**初级：数据基本查询**（能直接在年报中找到的）

需要利用LLM开源模型和上市公司年报原始数据，并以此为基础创建信息问答系统。系统需能够解决基本查询，如：某公司2021年的研发费用是多少？等问题。

**中级：数据统计分析查询**（需要做统计计算的）

在初级阶段的基础上，需要进行金融数据的统计分析和关联指标查询。系统需基于各类指标，提供问题和答案，如：某公司2021年研发费用增长率为多少？等问题。

**高级：开放性问题** （需要做分析总结的）

如：某公司2021年主要研发项目是否涉及国家创新领域，如新能源技术、人工智能等？

例如，我们可以根据公司年报信息回答如下问题：

![](image/image_Q0ELmghSPa.png)

![](image/image_qDUq1qb51L.png)

最终想要实现的QA系统问答能力应该如下：

![](image/image_egJqsGNorZ.png)

### 2、数据说明

**知识库构成：**

包含 11588 份 2019 年至 2021 年期间的部分上市公司年度报告，共70G。

**评测方法：**

评测任务的任务形式为：给定一组参考文档和问题，要求模型按照指定格式生成答案，测试集大小为1000条，问题包含多种类型。

问题示例：

{"ID": 1, "question": "2019年中国工商银行财务费用是多少元?"}

{"ID": 2, "question": "工商银行2019年营业外支出和营业外收入分别是多少元?" }

{"ID": 3, "question": "中国工商银行2021年净利润增长率是多少?保留2位小数。"}

答案示例：

{"ID": 1,

"question": "2019年中国工商银行财务费用是多少元?",

"answer":"2019年中国工商银行财务费用是12345678.9元。"}

{"ID": 2,

"question": "工商银行2019年营业外支出和营业外收入分别是多少元?",

"answer": "工商银行2019年营业外支出为12345678.9元，营业外收入为2345678.9元。"}

{"ID":3",

"question": "中国工商银行2021年净利润增长率是多少?保留2位小数。",

"answer": "中国工商银行2020年净利润为12345678.90元，2021年净利润为22345678.90元，根据公式，净利润增长率=(净利润-上年净利润)/上年净利润，得出结果中国工商银行2021年净利润增长率81.00%。" }

### 3、评测指标

评测方法如下：

![](image/image_IfHhxg4wb-.png)

评测示例：

{"question": "2019年中国工商银行财务费用是多少元?",

"prompt": {"财务费用": "12345678.9元", "key\_word":"财务费用、2019", "prom\_answer": "12345678.9元"},

"answer": \[

"2019年中国工商银行财务费用是12345678.9元。",

"2019年工商银行财务费用是12345678.9元。",

"中国工商银行2019年的财务费用是12345678.9元。" ]

}

评测计算示例：

输出结果：工商银行2019年财务费用是12345678.9元。

most similar sentences:

2019年工商银行财务费用是12345678.9元。 (Score: 0.9915)

中国工商银行2019年的财务费用是12345678.9元。 (Score: 0.9820)

2019年中国工商银行财务费用是12345678.9元。 (Score: 0.9720)

评分：0.25+0.25+0.9915\*0.5=0.9958分。

评分解释：prom\_answer正确、包含所有key\_word、相似度最高0.9915。

### 4、项目挑战

1、测试集问题种类多样，包含计算类、查询类、分析类等，单个框架难以处理

2、数据格式复杂，难以直接从pdf中抽取出规范化数据

3、没有微调数据集，人工标注数据成本高，如何高效的设计高质量的数据集

4、大模型生成不可控，难以保证输出结果的规范性

### 5、项目方案

![](image/image_w6w1NE1Nu9.png)

### 6、项目代码结构

![](image/image_JxSjM07RQw.png)

### 7、项目流程-PDF解析与信息抽取

年报中也有大量表格数据，如资产负债表等三大表，则需要通过其他方式提取数据，保存到关系型数据库中。

一般，我们将年报PDF文件进行格式转化。然后，基于TXT、HTML、DOCX或OCR识别的方式，提取其中的表格数据。

为了以后查询方便，项目将关系型数据（包括表格和基本信息），按照每家公司一行的方式，**做成一张大宽表**。这样可以避免跨表查询的复杂度。

解析与抽取步骤：

- **pdf文本抽取**: 采用pdf2text工具
- **页面召回**: 根据报表名称设置关键词找到对应的页
- **表格识别**: 利用camelot-py库实现基于图像识别的表格提取
- **信息过滤**: 非合并报表，调整报表，母公司报表

pdf信息抽取这部分代码在preprocess.py

**主要代码：**

&#x20;def extract\_pdf\_text():       提取文本信息

&#x20;def extract\_pdf\_tables()： 提取表格信息， 包括基本信息，雇员信息，研发人员表，合并资产负债表，合并利润表，合并现金流量表

表格提取过程如下：

![](image/image_Mc5GE6jkBo.png)

### 8、项目流程问题分类

要让大模型给出准确回答，前提是理解用户的问题和意图。针对不同类别的问题，采取不同的方法处理。

我们可以通过微调一个分类模型，来进行自动化的问题分类。实际上是构造一个“**分类器**”，步骤如下：

1. 首先，通过少样本in-context 学习生成数据集，并进行人工校验。
2. 然后，用该数据集进行微调（LoRA或P-Tuning）训练，构造问题分类模型。
3. 最后，把用户问题直接给到模型，可以让模型做选择题只给出类别。

问题分类任务通常涉及将文本分类到不同的预定义类别中。采取了以下步骤构建相应的问题模板：

- 定义类别：首先，明确定义任务中的类别。这些类别根据任务问题情况进行设计。以下是根据数据设计的问题分类：

![](image/image_B7dzJVA-Oc.png)

- 问题选项强化描述，增强AI对问题选项的理解，如加入分类的描述，分类相关的关键词等。以下是构造的问题分类的prompt：
  > 📌def \_get\_classify\_prompt(self, question) -> str:
  >
  > &#x20;   classify\_prompt = '''
  >
  > &#x20;   请问“{}”是属于下面哪个类别的问题?
  >
  > &#x20;   A: 公司基本信息,包含股票简称, 公司名称, 外文名称, 法定代表人, 注册地址, 办公地址, 公司网址网     站, 电子信箱等.
  >
  > &#x20;   B: 公司员工信息,包含员工人数, 员工专业, 员工类别, 员工教育程度等.
  >
  > &#x20;   C: 财务报表相关内容, 包含资产负债表, 现金流量表, 利润表 中存在的字段, 包括费用, 资产，金          额，收入等.
  >
  > &#x20;   D: 计算题,无法从年报中直接获得,需要根据计算公式获得, 包括增长率, 率, 比率, 比重, 占比等.&#x20;
  >
  > &#x20;   E: 统计题，需要从题目获取检索条件，在数据集/数据库中进行检索、过滤、排序后获得结果.&#x20;
  >
  > &#x20;   F: 开放性问题,包括介绍情况,介绍方法,分析情况,分析影响,什么是XXX.
  >
  > &#x20;   你只需要回答字母编号, 不要回答字母编号及选项文本外的其他内容.
  >
  > &#x20;   '''.format(question)
  >
  > &#x20;   return classify\_prompt
- 构建模板：根据问题集的问题遍历提取用户提问，为每个类别构建一个模板或规则集，以便根据问题文本分类到正确的类别中。

**问题分类流程：**

![](image/image_AFpQCAx2H_.png)

![](image/image_K-d_m4Q8wC.png)

**结果示例：**

![](image/image_FZ8esZcjcV.png)

**主要代码：**

模型微调：&#x20;

./ptuning/CLASSIFY\_PTUING/train.sh

在线预测：

main.py: Line 86\~L88

generate\_answer\_with\_classify.py: def do\_classification()

### 9、项目流程-SQL生成

NL2SQL微调训练重点任务还是构造训练数据集。这里必须指出，要针对实际数据查询场景构造数据集，而非使用Spider等通用的数据集。主要有两种方法：

1. 基于SQL模板生成数据集，但是泛化能力就比较低。
2. 构建SQL问答模板，对字段进行随机填充，然后利用ChatGPT等大模型对问题改写，生成数据集，效果相对更好。

无论使用哪一种方法，数据集必须经过人工校验。对于微调来说，数据的质量要求，远大于数据的数量要求。

如何生成正确的SQL语句，获取准确的数据呢？

总结了以下三点好的做法：

1. 使用**一张大宽表**，单表查询的SQL语句比较简单，生成SQL语句的准确率很高。
2. 在Prompt中可以**明确指定字段**，提升准确率。至于如何准确定位库表字段，可以采用前文提到的关键词匹配的方式。
3. 当遇到一些需要计算的场景，比如：营业利润率 = 营业利润 / 营业收入。此时最好**拆分问题**，生成多个简单的SQL查询语句，分别获取营业利润和营业收入后，再调用公式计算。

NL2SQL任务涉及将自然语言问题转化为SQL查询，以从数据库中检索信息。以下是构建NL2SQL模板的步骤：

- 定义查询类型：查询，排序，输出范围，计数，求和，单字段检索，多字段检索，多字段检索多字段，字段的过滤等SQL执行需求。
- 示例对话：催眠大语言模型进行Agent扮演Mysql数据库开发人员，通过自然语言问题和相应的SQL查询的【示例对话】提供指令的理解。
- 数据库字段：传入数据库表名，设计的字段，该数据库已经较好的清洗合并数据，同类型字段检索能有较好的性能。
- 生成SQL查询：根据SQL语法树，生成正确的的SQL执行语句。
- 参数替换：将自然语言问题中的参数（如列名、条件等）替换为存在库中的数据库元素。

> 📌role\_prompt = '''
>
> 你是一名Mysql数据库开发人员，你精通Mysql数据库的sql代码编写，你需要根据已知的表名、字段名和用户输入的问题编写sql代码
>
> 已知表名：company\_table
>
> 已知字段名：\[公司全称、年份、经营活动现金流入小计、公司的中文简称、固定资产...]
>
> 注意对问题中的中文数字（xx亿、xx千万、xx万）进行阿拉伯数字转换，如：一个亿、一亿需转换为100000000，一千万需转换为10000000
>
> 要求sql代码中的字段名必须是已知字段名，不得新增字段名
>
> 示例模板：
>
> """
>
> 用户输入：2019年哪家公司的负债合计最高？
>
> sql如下：
>
> \`\`\`sql&#x20;
>
> select 公司全称 from company\_table order by 负债合计 and 负债合计 is not null desc limit 1\`\`\`
>
> 用户输入：在上海注册的上市公司中，2019年谁的负债合计最高？金额是？
>
> sql如下：
>
> \`\`\`sql&#x20;
>
> select 公司全称, 负债合计 from company\_table where 注册地址 LIKE '%上海%' and 年份 = '2019' order by 负债合计 and 负债合计 is not null desc limit 1 \`\`\`
>
> """
>
> 请根据以下用户输入，输出sql代码。
>
> 用户输入：
>
> '''

> 📌question\_prompt = role\_prompt + question

**NL2SQL基本流程：**

![](image/image_P8jktoe2oH.png)

**NL2SQL结果示例：**

2019年负债总金额第7高的上市公司为？

```纯文本
 SELECT 公司全称 FROM company_table WHERE 年份 = '2019' AND 负债合计 IS NOT NULL ORDER BY 负债合计 DESC LIMIT 1 OFFSET 6
```

2020年其他流动资产最高的前三家上市公司是哪些家？

```纯文本
 SELECT 公司全称 FROM company_table WHERE 年份 = '2020' AND 其他流动资产 IS NOT NULL ORDER BY 其他流动资产 DESC LIMIT 3
```

2020年营业总收入最高的7家并且曾经在武汉注册的上市公司是？金额是？

```纯文本
 SELECT 公司全称, 营业总收入 FROM company_table WHERE 年份 = '2020' AND 注册地址 LIKE '%武汉%' AND 营业总收入 IS NOT NULL ORDER BY 营业总收入 DESC LIMIT 7
```

哪家上市公司2019年总负债最低？

```纯文本
SELECT 公司全称 FROM company_table WHERE 年份 = '2019' AND 负债合计 IS NOT NULL ORDER BY 负债合计 ASC LIMIT 1
```

哪家在上海注册的上市公司，2020年营业收入最高？金额是？

```纯文本
SELECT 公司全称, 营业收入 FROM company_table WHERE 年份 = '2020' AND 注册地址 LIKE '%上海%' AND 营业收入 IS NOT NULL ORDER BY 营业收入DESC LIMIT 1
```

**主要代码：**

模型微调：&#x20;

./ptuning/NL2SQL\_PTUING/train.sh

在线预测：

main.py: Line 96\~98

generate\_answer\_with\_classify.py: def do\_sql\_generation()

### 10、项目流程-关键词抽取

很多场景下，我们都需要通过Prompt，给到大模型来提取关键词。

比如，通过提取用户问题中的关键词，我们可以理解用户意图，再按照规则来匹配回答问题的方法。

但是对于专业领域，如果像ChatGLM2-6B这样的模型，不能很好地理解和准确提取专业术语，那么我们就可以微调一个关键词模型，用于在用户提问中提取相关专业的关键词。

在实际使用过程中，也可以先通过关键词模型对用户提问总结和提取出关键信息，然后将关键信息向量化，再进行向量检索匹配相应的关键词。我们还可以通过这样的方式，准确定位数据库表中的对应字段。

关键词提取任务涉及从文本中提取关键词或短语，有助于后面检索相关信息，越精准的提取越能够让返回的信息更加准确。以下是构建关键词提取模板的步骤：

- 构建关键词提取的prompt，需要不断尝试prompt对问题的处理和稳定性，非微调的情况下few-shot也能有合格的表现。以下是本项目构造的prompt形式：。

  role\_prompt = '''

  请帮我从以下句子中提取关键词。这些关键词是句子中最重要、最能概括句子主题的词汇。通过这些关键词，你可以更好地理解句子的内容。你只需要回答文本中的关键词，不要回答其他内容.

  用户输入：

  '''

  question\_prompt = role\_prompt + question
- 示例文本：收集不同具有代表性的问题所包含所需关键词类型的示例文本
- 构建模板：根据问题集的问题遍历提取用户提问，构建训练集的prompt模板

**构造的数据如下所示：**

![](image/image_aU_qUKUdek.png)

**输出结果示例：**

Question: 请告诉我山东华鲁恒升化工股份有限公司2020年的技术人员人数的具体数值

```纯文本
Keywords: 技术人员人数
```

Question: 请简要介绍2019年成都银河磁体股份有限公司面临退市情况。

```纯文本
Keywords: 面临退市情况
```

Question: 2020年湖南九典制药股份有限公司应收款项融资是多少元？

```纯文本
Keywords: 应收款项融资
```

Question: 在2020年的时候，厦工股份流动负债比率为多少？

```纯文本
Keywords: 流动负债比率
```

Question: 2020年广西博世科环保科技股份有限公司投资收益和净利润分别是多少元？

```纯文本
Keywords: '投资收益' , '净利润'
```

**主要代码：**

模型微调：&#x20;

./ptuning/KEYWORDS\_PTUING/train.sh

在线预测：

main.py: Line 91\~93

generate\_answer\_with\_classify.py: def do\_gen\_keywords()

### 11、项目流程-答案生成

**主要代码：**

main.py: Line 101\~102

generate\_answer\_with\_classify.py: def generate\_answer()

type1问题处理流程（基本信息类）:

**处理流程：**

![](image/image_mQalbVYLO5.png)

**回答示例：**

![](image/image_r6gpf2GwDB.png)

type2问题处理流程（统计计算类）:

**处理流程：**

![](image/image_tE6pr1dJy7.png)

**回答示例：**

![](image/image_XH6s6g9G0t.png)

type3问题处理流程（总结推理类）:

**处理流程：**

![](image/image_SvcrCO-yP0.png)

**回答示例：**

![](image/image_WLw7AEIfBQ.png)

### 12、项目总结

#### **Prompt设计**：

![](image/image_KIy-TkZvqB.png)

#### **模型微调：**

![](image/image_kcOoRtjT_l.png)

#### 微调的重要事项，以及如何解决常见问题

构造训练集的注意事项：

1. 数据准备： 确保训练集是高质量、多样化的。数据的质量对于模型的性能至关重要。
2. 数据清洗： 在将数据用于微调之前，可能需要进行数据清洗，包括去除噪声、错误标签或前后矛盾不一致的数据。6B模型的能力有限，尽可能把干扰项的格式和文本去除
3. 数据平衡： 确保正负样本的平衡性，特别是在二元分类问题中。不平衡的数据集可能导致模型性能下降。
4. 数据增强： 使用OpenAI的GPT4之类的大模型来增加训练数据的多样性，例如问题不同文案描述、使用不同的语气敬语、问题描述的语言组织先后顺序、不同金融名词的替换、一个问题使用不同数量的金融名词等。还可以一类问题扩增20-40条。这有助于模型泛化到不同类型的输入。
5. 样本选择： 根据任务类型，可以考虑在训练集中选择具有代表性的样本，以提高模型的性能。

训练过程中的注意事项：

1. 超参数调优： 仔细选择学习率、批量大小等超参数。
   - 使用学习率调度策略来帮助模型更好地收敛。P-Tuning v2默认学习率2e-2，可以从1e-2开始往下进行尝试，1e-5和5e-5是一个常见的起始学习率。
   - PRE\_SEQ\_LEN、max\_source\_length 这两个就注意要和训练集的max(question\_prompt)传入吻合，设置过小会导致文本被截断，设置过大会让训练变得很长，尽量设计为8的倍数（128、256、512、1024、2048）.
   - max\_target\_length 设置为最佳的输出范围即可。如本次问题分类和关键词提取使用256就足够（根据实际任务需要设定）。
   - 在默认配置 quantization\_bit=4、per\_device\_train\_batch\_size=1、gradient\_accumulation\_steps=16 下，INT4 的模型参数被冻结，一次训练迭代会以 1 的批处理大小进行 16 次累加的前后向传播，等效为 16 的总批处理大小，此时最低只需 6.7G 显存。不使用quantization\_bit=4参数的话，大概是15.7G 显存。若想在同等批处理大小下提升训练效率，可在二者乘积不变的情况下，加大 per\_device\_train\_batch\_size 的值，但也会带来更多的显存消耗，请根据实际情况酌情调整。
2. 监控训练： 定期记录训练过程中的指标，如损失、准确率、精确度和召回率。这有助于检测潜在问题并及时采取措施。
   - 比如训练过程中出现某个loss会突然调高，可能是训练集里面存在矛盾的标注（错误的标注）、存在同类问题不同标注方式（任务类型里类似关键词提取是最容易受到影响）
   - 300 step以内loss很难降到理想的数值（项目设定了0.02以下的loss目标）。需要检查训练集里面大概率存在一定量的错误标注，可以使用训练出来的微调权重跑一下evaluate看看结果generated\_predictions.txt的输出，看看哪些出现不稳定的情况。
   - 可以使用matplotlib将训练日志trainer\_state.json的loss进行输出打印
3. 过拟合处理： 如果观察到模型出现过拟合，一般是数据量太小，可以尝试把一类任务的训练集提到200条左右
4. 分布式训练： 对于大规模数据和模型训练，有条件的情况下，考虑使用分布式训练来加速训练过程。

#### **项目经验：**

- 大模型的泛化处理能力, 可以适应各种NLP任务, 文本分类/意图识别/实体提取/SQL生成等
- 指令识别能力+示例学习能力
- 行业大模型应用场景，要比ToC端的场景更加复杂。很难通过一个模型来解决，需要引入多个模型与工具。
- 6B的幻觉问题比较重，下游微调可以有效缓解。
- 大模型对微调数据质量要求非常高，重要性在数据量之上。
- 少量数据微调能得到非常不错的效果，（几百～几千条，人工就可梳理）
- &#x20;P-Tuning-v2（小数据集下，P-Tuning效果较好）
- 基于正则分类与抽取关键词的方式，实现数据库字段高频问题的快速回答
- 基于LLM抽取关键词的方式，实现与数据库字段相似问题的泛化
- QA系统搭建经验：精准问答走数据库nl2sql引擎，非精准问答走检索+LLM生成。
- 奥卡姆剃刀原则：能用规则解决的，尽量不用微调模型，能用小模型微调解决的，尽量不要用大模型。

#### **项目启示：**

**1. 行业应用场景分析与解构**，是最重要的前提。我认为甚至超过对技术的探索，因为你现在遇到的技术问题，也许很快通用大模型就能轻易解决。

**2. 复杂问题分解为简单问题**，是最关键的能力。形成问题链，逐一解决，“取其巧”来提升复杂问题回答的准确度，而不是一味依赖和等待大模型的能力。

**3. 质量要求远大于数量要求**，是最基本的认知。尤其是数据准备和构造微调训练数据集的过程中，一定要保证准确性。高质量数据的作用，也在最近各家预训练大模型的竞争中体现出来。

### 13、运行步骤

- **环境安装**

首先保证cuda, cudnn, pytorch-gpu的基本库是OK的，然后再安装requirements.txt的所需的第三方依赖。

后面在运行代码的过程中，根据某些缺失的个别包信息，再安装即可。

- **模型下载**

下载chatglm2-6b和Qwen1.5-7B模型，放到目录./data/pretrained_models/下。

数据：allpdf解压后，目录放到./data/下


- **模型微调**

如果想用lora微调，可以进入lora目录分别运行对应模型的训练，预测代码，下面以ptuing微调为例。

1.问题分类微调：

进入代码目录：cd ptuning/CLASSIFY\_PUTUNING

训练： bash train.sh

![](image/image_H0f2bXzsGf.png)

测试：bash evaluate.sh

观察指标预测情况，在output目录看看结果generated\_predictions.txt的输出，是否符合预期。

![](image/image_yBcrnC77ZZ.png)

2.关键词抽取微调：

进入代码目录：cd ptuning/KEYWORDS\_PUTUNING

训练： bash train.sh

测试：bash evaluate.sh

观察指标预测情况，在output目录看看结果generated\_predictions.txt的输出，是否符合预期。

![](image/image_zWBei0vbOP.png)

3.SQL生成微调：

进入代码目录：cd ptuning/NL2SQL\_PUTUNING

训练： bash train.sh

测试：bash evaluate.sh

![](image/image_sUM6QSLjf8.png)

观察指标预测情况，在output目录看看结果generated\_predictions.txt的输出，是否符合预期。

- **主代码运行**

回到项目根目录：

修改config/cfg.py下的BASE\_DIR为自己的项目的绝对路径。

python main.py

运行时间会较长，可根据服务器的cpu核数，调整config/cfg.py下的NUM\_PROCESSES设置，利用并行计算，加速模型处理过程。


- **指标计算**

python test\_score.py

预测指标在：data/test/output.json，包括type1, type2, type3以及加权后的总得分。

> 📌{"success": true, "score": 85.2515, "scoreJson": {"type1Score": 88.605, "type2Score": 91.3155, "type3-1Score": 66.5395, "type3-2Score": 88.3592, "score": 85.2515}}

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

主要代码说明：

main.py

- 包含全流程, 数据下载/pdf表格提取/表格⽣成/问题分类/问题关键词提取/SQL⽣成/问答结果⽣成/提交⽣成

generate\_answer\_with\_classify.py

- generate\_answer函数对每类问题⽣成问答结果

问答结果⽣成
Type1：

核心处理代码：

![](image/image_fOOij9ubL6.png)

主要处理步骤:

1. 对于Type1每类问题, 从该问题对应的报表中根据关键词召回可能对应的⾏
2. 对于⾏为字符串类型的, add\_text\_compare\_in\_table实现对⽐不同年份的字段内容是否相同
3. 对于问题问是否相同的, 直接返回召回的报表
4. 对于其他的, 通过prompt组合召回的报表来传给模型进⾏回答, 具体prompt函数⻅type1.get\_prompt

Type2：

核心处理代码：

![](image/image_mozMcgyRiz.png)

主要处理步骤:

1. 如果是计算增⻓率, 则需要召回前⼀年的数据
2. type2.get\_step\_questions将需要计算的内容转换为多个提问, 得到计算公式中的每个元素, 传给模型进⾏回答后, 提取数值作为公式的单元, 最后通过python计算该公式。

   a. 例如对于增⻓率, 公式为(A-B)/B, A表示当年的数值, B表示上年的数值

   b. 通过prompt\_util.get\_prompt\_single\_question⽣成针对A和B的提问

   c. 提取回答中的数值type2.get\_variable\_value\_from\_answer

   d. eval(formula)得到计算的结果

Type3：

核心处理代码：

![](image/image_duQK7CoxNi.png)

主要处理步骤:

1. question\_util.parse\_question\_keywords提取问题中的关键词
2. recall\_annual\_report\_texts进⾏⽂本的召回。其中召回的主要代码如下, 这⾥按⾏读取pdf⽂件中的⽂本内容, 采⽤bm25算法, 分别对问题关键词和原问题进⾏召回, 然后合并召回的⽂本块, 最后按照和原问题的字符匹配⻓度取匹配度最⾼的⽂本块:
3. prompt\_util.prompt\_question\_tp31组合召回的⽂本形成prompt传给模型进⾏回答

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

### 14、项目改进

1.利用gpt4或者文心一言等商用大模型，用提示工程的手法，将问题分类的训练集的样本量增强到原来的2-3倍，侧重query的增强，重新训练模型，观察效果变化。

2.将问题分类，关键词抽取，SQL生成的三个模型任选一个，利用llama-factory高效微调框架，将ptuning-v2微调改成Lora和GaLore微调，观察测试集效果的变化。

3.对于开放性问题，即问题类别F，改用向量检索+ES检索来召回Top-k文档。可以探索不同向量模型的选型，如 bge，stella等，并观察测试集效果的变化。

4.在Lora微调的基础上，大模型推理部分用FastLLM框架进行加速，观察加速前后完整处理全部测试集所用总时间，以及QPS的对比（可选）。

完成上述项目改进点，这些改进点都可以作为实战调优经验写进简历，上传改进后的项目代码到学习空间。

